{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXuFn1JuNPBn",
    "outputId": "035176bf-5e65-40a4-eeba-a6a30fa39ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Graph-Machine-Learning'...\n",
      "remote: Enumerating objects: 691, done.\u001B[K\n",
      "remote: Counting objects: 100% (99/99), done.\u001B[K\n",
      "remote: Compressing objects: 100% (77/77), done.\u001B[K\n",
      "remote: Total 691 (delta 38), reused 54 (delta 13), pack-reused 592 (from 1)\u001B[K\n",
      "Receiving objects: 100% (691/691), 85.31 MiB | 10.04 MiB/s, done.\n",
      "Resolving deltas: 100% (339/339), done.\n",
      "Updating files: 100% (133/133), done.\n"
     ]
    }
   ],
   "source": "!git clone https://@github.com/filippostaffoni/Graph-Machine-Learning.git"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKALJVBfNaFs",
    "outputId": "eb09f2a3-c1e4-4cd2-f504-8dabcfca1f47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Graph-Machine-Learning\n"
     ]
    }
   ],
   "source": [
    "%cd /content/Graph-Machine-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mA458gkhqCo2"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"filippostaffoni@hotmail.it\"\n",
    "!git config --global user.name \"Filippo Staffoni\"\n",
    "!git config pull.rebase false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sekvo44wqNHz",
    "outputId": "36e2735a-e564-4a8f-8511-8c46e97f48e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main ee83ebc] new models\n",
      " 7 files changed, 76 insertions(+), 4 deletions(-)\n",
      " create mode 100644 Models/VGAE/SAGE_1024_batchSize_layer_gelu_35nxn_embed_256_out.pth\n",
      " create mode 100644 Models/VGAE/with_embedding/with_pre_norm/with_residual_connections/1024_batchSize_layer_swiglu_35nxn_embed_128_out.pth\n",
      " create mode 100644 runs/new/VGAE_ogbl-biokg/SAGE_256d_20_epochs_0.001_lr_0.0005_weight_decay_35_num_neighbors_2_numLayer_layer/log.txt\n",
      " create mode 100644 runs/new/VGAE_ogbl-biokg/TB_SAGE_128d_20_epochs_0.001_lr_0.0005_weight_decay_35_num_neighbors_2_numLayer_layer/events.out.tfevents.1738332099.efdb27fea3a2.2297.1\n",
      " create mode 100644 runs/new/VGAE_ogbl-biokg/with_embedding/with_pre_norm/with_residual_connections/128d_20_epochs_0.001_lr_0.0005_weight_decay_35_num_neighbors_2_numLayer_layer/log.txt\n",
      " create mode 100644 runs/new/VGAE_ogbl-biokg/with_embedding/with_pre_norm/with_residual_connections/TB_128d_20_epochs_0.001_lr_0.0005_weight_decay_35_num_neighbors_2_numLayer_layer/events.out.tfevents.1738329292.efdb27fea3a2.2297.0\n",
      "Enumerating objects: 34, done.\n",
      "Counting objects: 100% (34/34), done.\n",
      "Delta compression using up to 2 threads\n",
      "Compressing objects: 100% (20/20), done.\n",
      "Writing objects: 100% (24/24), 560.93 KiB | 14.38 MiB/s, done.\n",
      "Total 24 (delta 5), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (5/5), completed with 4 local objects.\u001B[K\n",
      "To https://github.com/filippostaffoni/Graph-Machine-Learning.git\n",
      "   6d7e193..ee83ebc  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git add --all\n",
    "!git commit -m \"new models\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEMKrKz_OzI3",
    "outputId": "aaf0c57b-ad6f-4437-a5c1-1d416f7b72a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1+cu124\n",
      "Uninstalling torch-2.5.1+cu124:\n",
      "  Successfully uninstalled torch-2.5.1+cu124\n",
      "Found existing installation: torchvision 0.20.1+cu124\n",
      "Uninstalling torchvision-0.20.1+cu124:\n",
      "  Successfully uninstalled torchvision-0.20.1+cu124\n",
      "Found existing installation: torchaudio 2.5.1+cu124\n",
      "Uninstalling torchaudio-2.5.1+cu124:\n",
      "  Successfully uninstalled torchaudio-2.5.1+cu124\n",
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision==0.19.0\n",
      "  Downloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting torchaudio==2.4.0\n",
      "  Downloading torchaudio-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (11.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m797.3/797.3 MB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.0/7.0 MB\u001B[0m \u001B[31m75.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading torchaudio-2.4.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m63.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m50.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m50.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m56.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m176.2/176.2 MB\u001B[0m \u001B[31m6.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m9.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m209.4/209.4 MB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.1.0\n",
      "    Uninstalling triton-3.1.0:\n",
      "      Successfully uninstalled triton-3.1.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 torchaudio-2.4.0 torchvision-0.19.0 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "# Disinstallare la versione corrente di PyTorch\n",
    "!pip uninstall -y torch torchvision torchaudio\n",
    "\n",
    "# Installare PyTorch 2.4 e torchvision 0.19\n",
    "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6SrdcnETcIC",
    "outputId": "e64b5a5e-6185-4cdf-a8b9-c4212ea0a04e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/63.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.1/63.1 kB\u001B[0m \u001B[31m4.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.11)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2024.12.14)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m43.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.6.1\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
      "Collecting pyg_lib\n",
      "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/pyg_lib-0.4.0%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (2.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torch_scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_scatter-2.1.2%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (10.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.9/10.9 MB\u001B[0m \u001B[31m65.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torch_sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_sparse-0.6.18%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (5.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.1/5.1 MB\u001B[0m \u001B[31m23.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torch_cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_cluster-1.6.3%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m24.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torch_spline_conv\n",
      "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (989 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m989.8/989.8 kB\u001B[0m \u001B[31m928.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (1.26.4)\n",
      "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
      "Successfully installed pyg_lib-0.4.0+pt24cu121 torch_cluster-1.6.3+pt24cu121 torch_scatter-2.1.2+pt24cu121 torch_sparse-0.6.18+pt24cu121 torch_spline_conv-1.2.2+pt24cu121\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.8/88.8 kB\u001B[0m \u001B[31m8.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.9/56.9 kB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
    "!pip install umap-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aiF0MwJNSG9C",
    "outputId": "21f310de-9d9b-4212-ea9b-ced683ce8214"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\n",
      "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.6.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.3.0)\n",
      "Collecting outdated>=0.2.0 (from ogb)\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (75.1.0)\n",
      "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
      "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
      "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.8/78.8 kB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
      "Installing collected packages: littleutils, outdated, ogb\n",
      "Successfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eofkyGQrTw8z"
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "\n",
    "import torch.nn.functional as F  # Import functional API of PyTorch for activation functions, loss functions, etc.\n",
    "import torch.nn\n",
    "from torch.nn import BatchNorm1d, LayerNorm\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T  # Import geometric-specific transforms from PyTorch Geometric\n",
    "from torch_geometric.datasets import Planetoid  # Import Planetoid dataset class\n",
    "from torch_geometric.nn import SplineConv, GCNConv, GATConv, SAGEConv, DeepGraphInfomax, VGAE  # Import SplineConv layer from PyTorch Geometric\n",
    "from torch_geometric.typing import WITH_TORCH_SPLINE_CONV  # Check if SplineConv is available\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "import xgboost as xg\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import yaml\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.utils import dropout_edge\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GAE, VGAE\n",
    "import torch.nn as nn\n",
    "#from load_ogbl_biokg_homo import load_ogbl_biokg_homo as load_ogbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOlyjR6BbwFW"
   },
   "source": [
    "Carico il dataset ogbl-biokg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RBYl47gvdbgQ"
   },
   "outputs": [],
   "source": [
    "from torch_geometric import seed_everything\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ie8h-0qo3trw"
   },
   "outputs": [],
   "source": [
    "# data, _, drug_disease_set, drug_sideeffect_set = load_ogbl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeGG2w_2Qp_v"
   },
   "outputs": [],
   "source": [
    "# torch.save (data, 'load_data/data.pt')\n",
    "# torch.save (drug_disease_set, 'load_data/drug_disease_set.pt')\n",
    "# torch.save (drug_sideeffect_set, 'load_data/drug_sideeffect_set.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZRhZVoDQ_67",
    "outputId": "557a7f58-30f9-44ec-aefa-80fa79eefcd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-36ad64e024cb>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('load_data/data.pt')\n"
     ]
    }
   ],
   "source": [
    "data = torch.load('load_data/data.pt')\n",
    "# drug_disease_set = torch.load('load_data/drug_disease_set.pt')\n",
    "# drug_sideeffect_set = torch.load('load_data/drug_sideeffect_set.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaJyTR6rR4JH"
   },
   "outputs": [],
   "source": [
    "# dataset = PygLinkPropPredDataset(\"ogbl-biokg\",  root = 'dataset/')\n",
    "# data_for_debugging = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FodN17WydsbQ"
   },
   "outputs": [],
   "source": [
    "# data_for_debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCNCXoftVJmv"
   },
   "outputs": [],
   "source": [
    "# #creo lista one_hot_features, contenente 5 matrici con num righe = num nodi del tipo specificato e num colonne = num tipi di nodo\n",
    "# edge_attr = []\n",
    "# for edge_type in edge_types:\n",
    "#     num_edges = data.edge_index_dict[edge_type].shape[1]\n",
    "#     #torch.zeros((num_righe,num_colonne)) crea una matrice delle dimensioni indicate con tutti 0\n",
    "#     one_hot = torch.zeros((num_edges, num_edge_types))\n",
    "#     #per tutte le righe, nella colonna specificata metti 1\n",
    "#     one_hot[:, edge_type_to_idx[edge_type]] = 1\n",
    "#     #aggiungi matrice alla lista\n",
    "#     edge_attr.append(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctKuWkoPV-kZ"
   },
   "outputs": [],
   "source": [
    "#edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PPXUxKeGbj3f"
   },
   "outputs": [],
   "source": [
    "data.train_mask = data.val_mask = data.test_mask = None\n",
    "transform = T.RandomLinkSplit(num_val=0.1, num_test=0.1, is_undirected=False, add_negative_train_samples=False, neg_sampling_ratio = 1.0, split_labels = True)\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9g18UNhL9vOD"
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MipN0Unw3wwy"
   },
   "outputs": [],
   "source": [
    "# pos_mask = (train_data.edge_label == 1)\n",
    "# neg_mask = (train_data.edge_label == 0)\n",
    "# train_neg_edge_index = train_data.edge_label_index[:, neg_mask]\n",
    "# train_pos_edge_index = train_data.edge_label_index[:, pos_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIBdK1wd3wwy"
   },
   "outputs": [],
   "source": [
    "# pos_mask = (val_data.edge_label == 1)\n",
    "# neg_mask = (val_data.edge_label == 0)\n",
    "# val_neg_edge_index = val_data.edge_label_index[:, neg_mask]\n",
    "# val_pos_edge_index = val_data.edge_label_index[:, pos_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ujBg6C53wwz"
   },
   "outputs": [],
   "source": [
    "# pos_mask = (test_data.edge_label == 1)\n",
    "# neg_mask = (test_data.edge_label == 0)\n",
    "# test_neg_edge_index = test_data.edge_label_index[:, neg_mask]\n",
    "# test_pos_edge_index = test_data.edge_label_index[:, pos_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkiQB04J3wwz"
   },
   "outputs": [],
   "source": [
    "def check_overlap(edge_index1, edge_index2):\n",
    "    edges_set1 = set(map(tuple, edge_index1.t().tolist()))\n",
    "    edges_set2 = set(map(tuple, edge_index2.t().tolist()))\n",
    "\n",
    "    overlap = edges_set1.intersection(edges_set2)\n",
    "\n",
    "    return len(overlap),overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HZkID7z3wwz"
   },
   "outputs": [],
   "source": [
    "overlap_bool, overlap = check_overlap(train_data.pos_edge_label_index, val_data.pos_edge_label_index)\n",
    "print(overlap_bool)\n",
    "overlap_bool, overlap = check_overlap(train_data.pos_edge_label_index, test_data.pos_edge_label_index)\n",
    "print(overlap_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIgGYZgHYTDh"
   },
   "outputs": [],
   "source": [
    "val_data.edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5FEC9qbWTRX"
   },
   "outputs": [],
   "source": [
    "train_data.edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKl5RCWaQGjE"
   },
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "ic6OtcYaR5mX"
   },
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "M246fTxXZwbT"
   },
   "outputs": [],
   "source": [
    "batch_size = config['training']['batch_size']\n",
    "learning_rate = config['training']['learning_rate']\n",
    "weight_decay = config['training']['weight_decay']\n",
    "epochs = config['training']['epochs']\n",
    "dropout_edge_p = config['training']['dropout_edge_p']\n",
    "neighbors_per_node = config['data_loader']['neighbors_per_node']\n",
    "warmup = config['training']['warmup']\n",
    "warmup_steps = config['training']['warmup_steps']\n",
    "base_lr = config['training']['base_lr']\n",
    "model_name = config['gcn_encoder']['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "UbBryPQhR7Ls"
   },
   "outputs": [],
   "source": [
    "num_layers = config['gcn_encoder']['num_layers']\n",
    "normalization = config['gcn_encoder']['normalization']\n",
    "embedding_dim = config['gcn_encoder']['embedding_dim']\n",
    "output_dim = config['gcn_encoder']['output_dim']\n",
    "activation = config['gcn_encoder']['activation'].lower()\n",
    "use_embedding = config['gcn_encoder']['use_embedding']\n",
    "pre_norm = config['gcn_encoder']['pre_norm']\n",
    "residual_connections = config['gcn_encoder']['residual_connections']\n",
    "scale_factor = config['gcn_encoder']['scale_factor']\n",
    "layer_type = config[\"gcn_encoder\"][\"layer_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "iSsR3EKXd_-6",
    "outputId": "94f6354f-71c8-42ee-aff7-ca762b539dbe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'DGI'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "hsMx94UfSpPp"
   },
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "    \"relu\": torch.nn.ReLU(),\n",
    "    \"elu\": torch.nn.ELU(),\n",
    "    \"gelu\": torch.nn.GELU(),\n",
    "    \"swiglu\": torch.nn.SiLU()  # SiLU è equivalente a SwigLU in PyTorch\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7YvDMbOjQ78"
   },
   "outputs": [],
   "source": [
    "# class Encoder (torch.nn.Module):\n",
    "#   def __init__ (self, in_channels, out_channels):\n",
    "#     super(Encoder, self).__init__()\n",
    "#     self.conv1 = GCNConv (in_channels, out_channels * 2)\n",
    "#     self.bn1 = BatchNorm1d(out_channels * 2)\n",
    "#     self.conv2 = GCNConv (out_channels * 2, out_channels)\n",
    "#     self.bn2 = BatchNorm1d(out_channels)\n",
    "\n",
    "#   def forward (self, x , edge_index):\n",
    "#     x = F.relu(self.bn1(self.conv1 (x, edge_index)))\n",
    "#     x = self.bn2(self.conv2 (x,edge_index))\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "nnsRe6Bj03Qj"
   },
   "outputs": [],
   "source": [
    "def compute_new_dim(current_dim, output_dim, last):\n",
    "    if last:\n",
    "        return output_dim\n",
    "    if current_dim < output_dim:\n",
    "        return embedding_dim\n",
    "    if scale_factor == \"half\":\n",
    "        return int((current_dim + output_dim)/2)\n",
    "    else:\n",
    "        return current_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "G_7dC-igmbo6"
   },
   "outputs": [],
   "source": [
    "layer_types = {\n",
    "    \"GCN\": GCNConv,\n",
    "    \"SAGE\": SAGEConv,\n",
    "    \"GAT\": GATConv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPtxoDnlTwtg"
   },
   "outputs": [],
   "source": [
    "#per file yaml\n",
    "class GAEEncoder(torch.nn.Module):\n",
    "  def __init__(self, input_dim, embedding_dim, output_dim, num_layers, activation, normalization):\n",
    "        super(GAEEncoder, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.normalizations = torch.nn.ModuleList()\n",
    "        self.activation = activation_functions[activation]\n",
    "        if use_embedding:\n",
    "          self.class_embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        self.layers.append(GCNConv(embedding_dim, compute_new_dim(embedding_dim, output_dim)))\n",
    "        curr_dim = compute_new_dim(embedding_dim, output_dim)\n",
    "\n",
    "        if pre_norm and use_embedding:\n",
    "          if normalization == \"batch\":\n",
    "            self.pre_norm = BatchNorm1d(embedding_dim)\n",
    "          elif normalization == \"layer\":\n",
    "            self.pre_norm = LayerNorm(embedding_dim)\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "          if normalization == \"batch\":\n",
    "            self.normalizations.append(BatchNorm1d(curr_dim))\n",
    "          elif normalization == \"layer\":\n",
    "            self.normalizations.append(LayerNorm(curr_dim))\n",
    "          elif normalization == \"none\":\n",
    "            self.normalizations.append(None)\n",
    "          else:\n",
    "            raise ValueError(f\"Unsupported normalization type: {normalization}\")\n",
    "          self.layers.append(GCNConv(curr_dim, compute_new_dim(curr_dim, output_dim)))\n",
    "          curr_dim = compute_new_dim(curr_dim, output_dim)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    if use_embedding:\n",
    "      node_classes = torch.argmax(x, dim=1)\n",
    "      x = self.class_embedding(node_classes)\n",
    "    if pre_norm and use_embedding:\n",
    "      x = self.pre_norm(x)\n",
    "    if residual_connections:\n",
    "      for i, conv in enumerate(self.layers):\n",
    "          x = x + self.activation(conv(self.normalizations[i](x), edge_index))\n",
    "    else:\n",
    "      for i, conv in enumerate(self.layers):\n",
    "          x = conv(x, edge_index)\n",
    "      if i < len(self.layers) - 1:\n",
    "        if self.normalizations[i] is not None:  # Normalizzazione, se definita\n",
    "            x = self.normalizations[i](x)\n",
    "        x = self.activation(x)  # Attivazione\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Nru7oR3_bXBb"
   },
   "outputs": [],
   "source": [
    "#per file yaml\n",
    "class VGAEEncoder(torch.nn.Module):\n",
    "  def __init__(self, input_dim, embedding_dim, output_dim, num_layers, activation, normalization):\n",
    "        super(VGAEEncoder, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.normalizations = torch.nn.ModuleList()\n",
    "        self.activation = activation_functions[activation]\n",
    "        self.ConvLayer = layer_types[layer_type]\n",
    "        last = False\n",
    "\n",
    "        if use_embedding:\n",
    "          self.class_embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "          input_dim = embedding_dim\n",
    "\n",
    "        self.layers.append(self.ConvLayer(input_dim, compute_new_dim(input_dim, output_dim, last)))\n",
    "        curr_dim = compute_new_dim(input_dim, output_dim, last)\n",
    "        if pre_norm and use_embedding:\n",
    "          if normalization == \"batch\":\n",
    "            self.pre_norm = BatchNorm1d(embedding_dim)\n",
    "          elif normalization == \"layer\":\n",
    "            self.pre_norm = LayerNorm(embedding_dim)\n",
    "        for i in range(num_layers - 1):\n",
    "          if normalization == \"batch\":\n",
    "            self.normalizations.append(BatchNorm1d(curr_dim))\n",
    "          elif normalization == \"layer\":\n",
    "            self.normalizations.append(LayerNorm(curr_dim))\n",
    "          elif normalization == \"none\":\n",
    "            self.normalizations.append(None)\n",
    "          else:\n",
    "            raise ValueError(f\"Unsupported normalization type: {normalization}\")\n",
    "        if residual_connections:\n",
    "          if normalization == \"batch\":\n",
    "            self.normalizations.append(BatchNorm1d(curr_dim))\n",
    "          elif normalization == \"layer\":\n",
    "            self.normalizations.append(LayerNorm(curr_dim))\n",
    "          if (i == num_layers - 2):\n",
    "            last = True\n",
    "          self.layers.append(self.ConvLayer(curr_dim, compute_new_dim(curr_dim, output_dim, last)))\n",
    "          curr_dim = compute_new_dim(curr_dim, output_dim, last)\n",
    "\n",
    "        self.conv_mu = self.ConvLayer(curr_dim, int(curr_dim / 2))\n",
    "        self.conv_logstd = self.ConvLayer(curr_dim, int(curr_dim / 2))\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    if use_embedding:\n",
    "      node_classes = torch.argmax(x, dim=1)\n",
    "      x = self.class_embedding(node_classes)\n",
    "    if pre_norm and use_embedding:\n",
    "      x = self.pre_norm(x)\n",
    "    if residual_connections:\n",
    "      for i, conv in enumerate(self.layers):\n",
    "          x = x + self.activation(conv(self.normalizations[i](x), edge_index))\n",
    "    else:\n",
    "      for i, conv in enumerate(self.layers):\n",
    "          x = conv(x, edge_index)\n",
    "      if i < len(self.layers) - 1:\n",
    "        if self.normalizations[i] is not None:  # Normalizzazione, se definita\n",
    "            x = self.normalizations[i](x)\n",
    "        x = self.activation(x)  # Attivazione\n",
    "    return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sv2FR4MJ9iRj"
   },
   "outputs": [],
   "source": [
    "def corruption(x, edge_index):\n",
    "    return x[torch.randperm(x.size(0), device=x.device)], edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "msQ5B3qMVaHE"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, GCNConv):\n",
    "        torch.nn.init.xavier_uniform_(m.lin.weight)\n",
    "        if m.lin.bias is not None:\n",
    "            torch.nn.init.zeros_(m.lin.bias)\n",
    "    elif isinstance(m, (BatchNorm1d, LayerNorm)):\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            torch.nn.init.ones_(m.weight)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, torch.nn.Embedding):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nI2WlODjeNDf"
   },
   "outputs": [],
   "source": [
    "num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1CBueafaZPA"
   },
   "outputs": [],
   "source": [
    "model = GAE(GAEEncoder(\n",
    "    input_dim=data.num_features,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=num_layers,\n",
    "    activation=activation,\n",
    "    normalization=normalization\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "hUARGyJUeuzx"
   },
   "outputs": [],
   "source": [
    "model = VGAE(VGAEEncoder(\n",
    "    input_dim=data.num_features,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=num_layers,\n",
    "    activation=activation,\n",
    "    normalization=normalization\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "WlYqj0wb9ps2",
    "outputId": "76a1730b-047b-45c1-8462-4d9735f5e8c6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GAEEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-29-2856f9c87fea>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m model = DeepGraphInfomax(\n\u001B[0;32m----> 2\u001B[0;31m     encoder=GAEEncoder(\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0minput_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_features\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0membedding_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0membedding_dim\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mhidden_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhidden_dim\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'GAEEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "model = DeepGraphInfomax(\n",
    "    encoder=GAEEncoder(\n",
    "    input_dim=data.num_features,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=num_layers,\n",
    "    activation=activation,\n",
    "    normalization=normalization),\n",
    "    hidden_channels=output_dim,\n",
    "    summary=lambda z, *args, **kwargs: z.mean(dim=0).sigmoid(), #Calcola la media di tutti gli embedding dei nodi lungo la dimensione dei nodi (dim=0), ottenendo una rappresentazione globale del grafo.\n",
    "    corruption=corruption,\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7IxVXLAV29F",
    "outputId": "f216bc64-509d-4bf6-a287-51b5b03630c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGAE(\n",
       "  (encoder): VGAEEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): SAGEConv(5, 256, aggr=mean)\n",
       "      (1): SAGEConv(256, 128, aggr=mean)\n",
       "    )\n",
       "    (normalizations): ModuleList(\n",
       "      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (activation): GELU(approximate='none')\n",
       "    (conv_mu): SAGEConv(128, 64, aggr=mean)\n",
       "    (conv_logstd): SAGEConv(128, 64, aggr=mean)\n",
       "  )\n",
       "  (decoder): InnerProductDecoder()\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffEPiZsufK48",
    "outputId": "d883e122-6e87-4c2d-a201-821b829cd855"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGAE(\n",
       "  (encoder): VGAEEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): GCNConv(512, 256)\n",
       "      (1): GCNConv(256, 128)\n",
       "    )\n",
       "    (normalizations): ModuleList(\n",
       "      (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (activation): GELU(approximate='none')\n",
       "    (class_embedding): Embedding(5, 512)\n",
       "    (conv_mu): GCNConv(128, 64)\n",
       "    (conv_logstd): GCNConv(128, 64)\n",
       "  )\n",
       "  (decoder): InnerProductDecoder()\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "ufacSC3YfrlU"
   },
   "outputs": [],
   "source": [
    "device = torch.device ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "x = data.x.to(device)\n",
    "train_pos_edge_index = train_data.pos_edge_label_index.to(device)\n",
    "valid_pos_edge_index = val_data.pos_edge_label_index.to(device)\n",
    "test_pos_edge_index = test_data.pos_edge_label_index.to(device)\n",
    "valid_neg_edge_index = val_data.neg_edge_label_index.to(device)\n",
    "test_neg_edge_index = test_data.neg_edge_label_index.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBosVAohkwB9"
   },
   "outputs": [],
   "source": [
    "# #parametres\n",
    "# out_channels = 128\n",
    "# num_features = data.num_features\n",
    "# epochs = 100\n",
    "\n",
    "# #model\n",
    "# model = GAE(Encoder (num_features, out_channels))\n",
    "\n",
    "# #todevice\n",
    "# device = torch.device ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "# x = data.x.to(device)\n",
    "# train_pos_edge_index = train_data.pos_edge_label_index.to(device)\n",
    "# valid_pos_edge_index = val_data.pos_edge_label_index.to(device)\n",
    "# test_pos_edge_index = test_data.pos_edge_label_index.to(device)\n",
    "# valid_neg_edge_index = val_data.neg_edge_label_index.to(device)\n",
    "# test_neg_edge_index = test_data.neg_edge_label_index.to(device)\n",
    "\n",
    "# #optimizer\n",
    "# lr = 0.001\n",
    "# weight_decay = 5e-4\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIC9yw0TAA_3",
    "outputId": "8b461c4d-4b12-472c-b7a0-09f2ce7fecf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = NeighborLoader(\n",
    "    data=train_data.to(device), #VGAE GAE\n",
    "    #data=data.to(device), #DGI\n",
    "    num_neighbors=[neighbors_per_node] * num_layers,  # Numero di vicini a ciascun hop per ogni nodo nel batch\n",
    "    batch_size= batch_size,           # Numero di nodi iniziali in ciascun batch\n",
    "    shuffle=True,\n",
    "    num_workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "PzbFYi76J95x"
   },
   "outputs": [],
   "source": [
    "train_edges_set = set(map(tuple, train_data.pos_edge_label_index.T.tolist()))\n",
    "val_edges_set = set(map(tuple, val_data.pos_edge_label_index.T.tolist()))\n",
    "test_edges_set = set(map(tuple, test_data.pos_edge_label_index.T.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "HDOa0srTfkeR"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "RZs5K0C8YUsV"
   },
   "outputs": [],
   "source": [
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return (step / warmup_steps)  # Interpolazione lineare\n",
    "    return 1.0  # Dopo il warm-up, mantieni il target LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "-w151q24YZ30"
   },
   "outputs": [],
   "source": [
    "if warmup:\n",
    "  scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yeja9XlldMh"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  steps= 0\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    edge_index_dropped, _ = dropout_edge(\n",
    "            batch.edge_index,\n",
    "            p=dropout_edge_p\n",
    "        )\n",
    "    z = model.encode(batch.x, edge_index_dropped)\n",
    "    neg_edge_index = negative_sampling(\n",
    "    edge_index=batch.edge_index,\n",
    "    num_neg_samples=batch.edge_index.size(1),\n",
    "    )\n",
    "    neg_edges_list = [edge for edge in neg_edge_index.T.tolist() if tuple(edge) not in train_edges_set and tuple(edge) not in val_edges_set and tuple(edge) not in test_edges_set]\n",
    "    neg_edge_index = torch.tensor(neg_edges_list).T\n",
    "    loss = model.recon_loss(z, batch.edge_index, neg_edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if warmup:\n",
    "      scheduler.step()\n",
    "      #current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch: {epoch}, Step: {steps}, loss: {loss:.4f}\")\n",
    "    #log_step_performance (epoch, steps, current_lr, loss, epochs, embedding_dim, learning_rate, weight_decay, neighbors_per_node, num_layers, normalization)\n",
    "    steps += 1\n",
    "    total_loss += loss.item()\n",
    "  return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "UlXthk5Lf4vv"
   },
   "outputs": [],
   "source": [
    "def train_VGAE():\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  steps= 0\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    edge_index_dropped, _ = dropout_edge(\n",
    "            batch.edge_index,\n",
    "            p=dropout_edge_p\n",
    "        )\n",
    "    z = model.encode(batch.x, edge_index_dropped)\n",
    "    neg_edge_index = negative_sampling(\n",
    "    edge_index=batch.edge_index,\n",
    "    num_neg_samples=batch.edge_index.size(1),\n",
    "    )\n",
    "    neg_edges_list = [edge for edge in neg_edge_index.T.tolist() if tuple(edge) not in train_edges_set and tuple(edge) not in val_edges_set and tuple(edge) not in test_edges_set]\n",
    "    neg_edge_index = torch.tensor(neg_edges_list).T\n",
    "    loss = model.recon_loss(z, batch.edge_index, neg_edge_index)\n",
    "    num_nodes = batch.x.size(0)\n",
    "    kl_loss = model.kl_loss() / num_nodes\n",
    "    loss = loss + kl_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if warmup:\n",
    "      scheduler.step()\n",
    "      current_lr = scheduler.get_last_lr()[0]\n",
    "      print(f'LR: {current_lr}')\n",
    "    print(f\"Epoch: {epoch}, Step: {steps}, loss: {loss:.4f}\")\n",
    "    #log_step_performance (epoch, steps, current_lr, loss, epochs, embedding_dim, learning_rate, weight_decay, neighbors_per_node, num_layers, normalization)\n",
    "    steps += 1\n",
    "    total_loss += loss.item()\n",
    "  return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-X_CvNYY-SqJ"
   },
   "outputs": [],
   "source": [
    "def train_DGI():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    steps= 0\n",
    "    for batch in train_loader:\n",
    "      optimizer.zero_grad()\n",
    "      edge_index_dropped, _ = dropout_edge(\n",
    "          batch.edge_index,\n",
    "          p=dropout_edge_p\n",
    "      )\n",
    "      pos_z, neg_z, summary = model(batch.x, batch.edge_index)\n",
    "      loss = model.loss(pos_z, neg_z, summary)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      current_lr = scheduler.get_last_lr()[0]\n",
    "      print(f\"Epoch: {epoch}, Step: {steps}, LR: {current_lr:.6f}, loss: {loss:.4f}\")\n",
    "      log_step_performance (epoch, steps, current_lr, loss, \"10\", output_dim, learning_rate, weight_decay, neighbors_per_node, num_layers, normalization)\n",
    "      steps += 1\n",
    "      total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# def valid_DGI():\n",
    "#   model.eval()\n",
    "#   with torch.no_grad():\n",
    "#     pos_z, neg_z, summary = model(batch.x, batch.edge_index)\n",
    "#     valid_loss = model.loss(z[valid_pos_edge_index], z[valid_neg_edge_index])\n",
    "#   return valid_loss\n",
    "\n",
    "# def test_DGI():\n",
    "#     model.eval()\n",
    "#     z, _, _ = model(data.x, data.edge_index)\n",
    "#     acc = model.test(z[data.train_mask], data.y[data.train_mask],\n",
    "#                      z[data.test_mask], data.y[data.test_mask], max_iter=150)\n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "YBqRrCde_jvc"
   },
   "outputs": [],
   "source": [
    "def valid(pos_edge_index, neg_edge_index):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    z = model.encode(x, train_pos_edge_index)\n",
    "    #neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\n",
    "    AUC, ap = model.test(z, pos_edge_index, neg_edge_index)\n",
    "    valid_loss = model.recon_loss(z, pos_edge_index, neg_edge_index)\n",
    "  return AUC, ap, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "dQyXCJB-C7G-"
   },
   "outputs": [],
   "source": [
    "def test(pos_edge_index, neg_edge_index):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    z = model.encode(x, test_data.edge_index.to(device))\n",
    "    #neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\n",
    "  return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "AJUHu1YKpMQ-",
    "outputId": "6427c37d-f628-4c0a-9ecd-69825a754bd6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'layer'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "Fh3glbi2FbtZ"
   },
   "outputs": [],
   "source": [
    "def log_training_parameters(batch_size, learning_rate, weight_decay, epochs, neighbors_per_node, num_layers, output_dim, activation, normalization):\n",
    "  folder_path = f'runs/new/{model_name}_ogbl-biokg/'\n",
    "  if use_embedding:\n",
    "    folder_path = os.path.join(folder_path, 'with_embedding/')\n",
    "  if pre_norm:\n",
    "    folder_path = os.path.join(folder_path, 'with_pre_norm/')\n",
    "  if residual_connections:\n",
    "    folder_path = os.path.join(folder_path, 'with_residual_connections/')\n",
    "  folder_path = os.path.join(folder_path, f'{layer_type}_{output_dim}d_{epochs}_epochs_{learning_rate}_lr_{weight_decay}_weight_decay_{neighbors_per_node}_num_neighbors_{num_layers}_numLayer_{normalization}')\n",
    "  if not os.path.exists(folder_path):\n",
    "      os.makedirs(folder_path, exist_ok=True)\n",
    "  file_path = os.path.join(folder_path, 'log.txt')\n",
    "  with open(file_path, \"a\") as log_file:\n",
    "    # Scrivi i parametri una sola volta all'inizio del training\n",
    "    log_file.write(f\"\\n=== Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')} ===\\n\")\n",
    "    log_file.write(f\"Batch size: {batch_size}\\n\")\n",
    "    log_file.write(f\"Learning Rate: {learning_rate}\\n\")\n",
    "    log_file.write(f\"Weight Decay: {weight_decay}\\n\")\n",
    "    log_file.write(f\"Epochs: {epochs}\\n\")\n",
    "    log_file.write(f\"Neighbors loader: {neighbors_per_node}\\n\")\n",
    "    log_file.write(f\"Number of layers: {num_layers}\\n\")\n",
    "    log_file.write(f\"output dim: {output_dim}\\n\")\n",
    "    log_file.write(f\"Activation: {activation}\\n\")\n",
    "    log_file.write(f\"Normalization: {normalization}\\n\")\n",
    "    log_file.write(f\"Optimizer: Adam\\n\")\n",
    "    log_file.write(f\"Pre norm and residual connections: {pre_norm} and {residual_connections}\\n\")\n",
    "    log_file.write(f\"=============================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "TTrH4yGDEnyb"
   },
   "outputs": [],
   "source": [
    "def log_epoch_performance(epoch, train_loss, loss_valid, AUC, ap, epochs, output_dim, learning_rate, weight_decay, neighbors_per_node, num_layers, normalization):\n",
    "  folder_path = f'runs/new/{model_name}_ogbl-biokg/'\n",
    "  if use_embedding:\n",
    "    folder_path = os.path.join(folder_path, 'with_embedding/')\n",
    "  if pre_norm:\n",
    "    folder_path = os.path.join(folder_path, 'with_pre_norm/')\n",
    "  if residual_connections:\n",
    "    folder_path = os.path.join(folder_path, 'with_residual_connections/')\n",
    "  folder_path = os.path.join(folder_path, f'{layer_type}_{output_dim}d_{epochs}_epochs_{learning_rate}_lr_{weight_decay}_weight_decay_{neighbors_per_node}_num_neighbors_{num_layers}_numLayer_{normalization}')\n",
    "  if not os.path.exists(folder_path):\n",
    "      os.makedirs(folder_path, exist_ok=True)\n",
    "  file_path = os.path.join(folder_path, 'log.txt')\n",
    "  with open(file_path, \"a\") as log_file:\n",
    "    log_file.write(f\"Epoch: {epoch:}, train_Loss: {train_loss:.4f}, valid_loss: {loss_valid:.4f}, AUC: {AUC:.4f}, ap: {ap:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "UzRhkvRlZAxW"
   },
   "outputs": [],
   "source": [
    "def log_step_performance(epoch, step, curr_lr, loss, epochs, output_dim, learning_rate, weight_decay, neighbors_per_node, num_layers, normalization):\n",
    "  folder_path = f'runs/new/{model_name}_ogbl-biokg/'\n",
    "  if use_embedding:\n",
    "    folder_path = os.path.join(folder_path, 'with_embedding/')\n",
    "  if pre_norm:\n",
    "    folder_path = os.path.join(folder_path, 'with_pre_norm/')\n",
    "  if residual_connections:\n",
    "    folder_path = os.path.join(folder_path, 'with_residual_connections/')\n",
    "  folder_path = os.path.join(folder_path, f'{layer_type}_{output_dim}d_{epochs}_epochs_{learning_rate}_lr_{weight_decay}_weight_decay_{neighbors_per_node}_num_neighbors_{num_layers}_numLayer_{normalization}')\n",
    "  if not os.path.exists(folder_path):\n",
    "      os.makedirs(folder_path, exist_ok=True)\n",
    "  file_path = os.path.join(folder_path, 'log.txt')\n",
    "  with open(file_path, \"a\") as log_file:\n",
    "    log_file.write(f\"Epoch: {epoch}, Step: {step}, LR: {curr_lr:.6f}, loss: {loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "79fPTOlsy1As"
   },
   "outputs": [],
   "source": [
    "folder_path_2 = f'runs/new/{model_name}_ogbl-biokg/'\n",
    "if use_embedding:\n",
    "  folder_path_2 = os.path.join(folder_path_2, 'with_embedding/')\n",
    "if pre_norm:\n",
    "  folder_path_2 = os.path.join(folder_path_2, 'with_pre_norm/')\n",
    "if residual_connections:\n",
    "  folder_path_2 = os.path.join(folder_path_2, 'with_residual_connections/')\n",
    "folder_path_2 = os.path.join(folder_path_2, f'TB_{layer_type}_{output_dim}d_{epochs}_epochs_{learning_rate}_lr_{weight_decay}_weight_decay_{neighbors_per_node}_num_neighbors_{num_layers}_numLayer_{normalization}')\n",
    "writer = SummaryWriter(folder_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W78IcjREpK61"
   },
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#   batch_prova = batch\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY575RL6PwBh"
   },
   "outputs": [],
   "source": [
    "# batch_prova.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6WCp7tHD-oE"
   },
   "outputs": [],
   "source": [
    "model_name = \"VGAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSyzCFXJmyxA",
    "outputId": "16f65064-af83-4cd0-892d-875649c1b250"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 4e-06\n",
      "Epoch: 1, Step: 0, loss: 7.6573\n",
      "LR: 8e-06\n",
      "Epoch: 1, Step: 1, loss: 5.8374\n",
      "LR: 1.2e-05\n",
      "Epoch: 1, Step: 2, loss: 4.9205\n",
      "LR: 1.6e-05\n",
      "Epoch: 1, Step: 3, loss: 4.3319\n",
      "LR: 2e-05\n",
      "Epoch: 1, Step: 4, loss: 3.7890\n",
      "LR: 2.4e-05\n",
      "Epoch: 1, Step: 5, loss: 3.2750\n",
      "LR: 2.8e-05\n",
      "Epoch: 1, Step: 6, loss: 2.8787\n",
      "LR: 3.2e-05\n",
      "Epoch: 1, Step: 7, loss: 2.5422\n",
      "LR: 3.6e-05\n",
      "Epoch: 1, Step: 8, loss: 2.2768\n",
      "LR: 4e-05\n",
      "Epoch: 1, Step: 9, loss: 2.0090\n",
      "LR: 4.4e-05\n",
      "Epoch: 1, Step: 10, loss: 1.7894\n",
      "LR: 4.8e-05\n",
      "Epoch: 1, Step: 11, loss: 1.6395\n",
      "LR: 5.2e-05\n",
      "Epoch: 1, Step: 12, loss: 1.4951\n",
      "LR: 5.6e-05\n",
      "Epoch: 1, Step: 13, loss: 1.3854\n",
      "LR: 6e-05\n",
      "Epoch: 1, Step: 14, loss: 1.3055\n",
      "LR: 6.4e-05\n",
      "Epoch: 1, Step: 15, loss: 1.2428\n",
      "LR: 6.800000000000001e-05\n",
      "Epoch: 1, Step: 16, loss: 1.1965\n",
      "LR: 7.2e-05\n",
      "Epoch: 1, Step: 17, loss: 1.1555\n",
      "LR: 7.6e-05\n",
      "Epoch: 1, Step: 18, loss: 1.1274\n",
      "LR: 8e-05\n",
      "Epoch: 1, Step: 19, loss: 1.1061\n",
      "LR: 8.400000000000001e-05\n",
      "Epoch: 1, Step: 20, loss: 1.0903\n",
      "LR: 8.8e-05\n",
      "Epoch: 1, Step: 21, loss: 1.0804\n",
      "LR: 9.2e-05\n",
      "Epoch: 1, Step: 22, loss: 1.0701\n",
      "LR: 9.6e-05\n",
      "Epoch: 1, Step: 23, loss: 1.0564\n",
      "LR: 0.0001\n",
      "Epoch: 1, Step: 24, loss: 1.0487\n",
      "LR: 0.000104\n",
      "Epoch: 1, Step: 25, loss: 1.0456\n",
      "LR: 0.000108\n",
      "Epoch: 1, Step: 26, loss: 1.0417\n",
      "LR: 0.000112\n",
      "Epoch: 1, Step: 27, loss: 1.0261\n",
      "LR: 0.00011600000000000001\n",
      "Epoch: 1, Step: 28, loss: 1.0269\n",
      "LR: 0.00012\n",
      "Epoch: 1, Step: 29, loss: 1.0232\n",
      "LR: 0.000124\n",
      "Epoch: 1, Step: 30, loss: 1.0230\n",
      "LR: 0.000128\n",
      "Epoch: 1, Step: 31, loss: 1.0165\n",
      "LR: 0.000132\n",
      "Epoch: 1, Step: 32, loss: 1.0086\n",
      "LR: 0.00013600000000000003\n",
      "Epoch: 1, Step: 33, loss: 1.0160\n",
      "LR: 0.00014000000000000001\n",
      "Epoch: 1, Step: 34, loss: 1.0112\n",
      "LR: 0.000144\n",
      "Epoch: 1, Step: 35, loss: 1.0110\n",
      "LR: 0.000148\n",
      "Epoch: 1, Step: 36, loss: 1.0023\n",
      "LR: 0.000152\n",
      "Epoch: 1, Step: 37, loss: 1.0073\n",
      "LR: 0.000156\n",
      "Epoch: 1, Step: 38, loss: 0.9997\n",
      "LR: 0.00016\n",
      "Epoch: 1, Step: 39, loss: 1.0026\n",
      "LR: 0.000164\n",
      "Epoch: 1, Step: 40, loss: 1.0046\n",
      "LR: 0.00016800000000000002\n",
      "Epoch: 1, Step: 41, loss: 0.9963\n",
      "LR: 0.00017199999999999998\n",
      "Epoch: 1, Step: 42, loss: 0.9959\n",
      "LR: 0.000176\n",
      "Epoch: 1, Step: 43, loss: 0.9942\n",
      "LR: 0.00017999999999999998\n",
      "Epoch: 1, Step: 44, loss: 1.0004\n",
      "LR: 0.000184\n",
      "Epoch: 1, Step: 45, loss: 0.9939\n",
      "LR: 0.00018800000000000002\n",
      "Epoch: 1, Step: 46, loss: 0.9936\n",
      "LR: 0.000192\n",
      "Epoch: 1, Step: 47, loss: 0.9935\n",
      "LR: 0.00019600000000000002\n",
      "Epoch: 1, Step: 48, loss: 0.9903\n",
      "LR: 0.0002\n",
      "Epoch: 1, Step: 49, loss: 0.9889\n",
      "LR: 0.000204\n",
      "Epoch: 1, Step: 50, loss: 0.9974\n",
      "LR: 0.000208\n",
      "Epoch: 1, Step: 51, loss: 0.9923\n",
      "LR: 0.000212\n",
      "Epoch: 1, Step: 52, loss: 0.9908\n",
      "LR: 0.000216\n",
      "Epoch: 1, Step: 53, loss: 0.9894\n",
      "LR: 0.00022\n",
      "Epoch: 1, Step: 54, loss: 0.9866\n",
      "LR: 0.000224\n",
      "Epoch: 1, Step: 55, loss: 0.9821\n",
      "LR: 0.000228\n",
      "Epoch: 1, Step: 56, loss: 0.9863\n",
      "LR: 0.00023200000000000003\n",
      "Epoch: 1, Step: 57, loss: 0.9891\n",
      "LR: 0.000236\n",
      "Epoch: 1, Step: 58, loss: 0.9845\n",
      "LR: 0.00024\n",
      "Epoch: 1, Step: 59, loss: 0.9900\n",
      "LR: 0.000244\n",
      "Epoch: 1, Step: 60, loss: 0.9825\n",
      "LR: 0.000248\n",
      "Epoch: 1, Step: 61, loss: 0.9945\n",
      "LR: 0.000252\n",
      "Epoch: 1, Step: 62, loss: 0.9776\n",
      "LR: 0.000256\n",
      "Epoch: 1, Step: 63, loss: 0.9864\n",
      "LR: 0.00026000000000000003\n",
      "Epoch: 1, Step: 64, loss: 0.9845\n",
      "LR: 0.000264\n",
      "Epoch: 1, Step: 65, loss: 0.9849\n",
      "LR: 0.000268\n",
      "Epoch: 1, Step: 66, loss: 0.9843\n",
      "LR: 0.00027200000000000005\n",
      "Epoch: 1, Step: 67, loss: 0.9853\n",
      "LR: 0.00027600000000000004\n",
      "Epoch: 1, Step: 68, loss: 0.9882\n",
      "LR: 0.00028000000000000003\n",
      "Epoch: 1, Step: 69, loss: 0.9785\n",
      "LR: 0.00028399999999999996\n",
      "Epoch: 1, Step: 70, loss: 0.9815\n",
      "LR: 0.000288\n",
      "Epoch: 1, Step: 71, loss: 0.9827\n",
      "LR: 0.000292\n",
      "Epoch: 1, Step: 72, loss: 0.9802\n",
      "LR: 0.000296\n",
      "Epoch: 1, Step: 73, loss: 0.9789\n",
      "LR: 0.0003\n",
      "Epoch: 1, Step: 74, loss: 0.9848\n",
      "LR: 0.000304\n",
      "Epoch: 1, Step: 75, loss: 0.9768\n",
      "LR: 0.000308\n",
      "Epoch: 1, Step: 76, loss: 0.9825\n",
      "LR: 0.000312\n",
      "Epoch: 1, Step: 77, loss: 0.9753\n",
      "LR: 0.000316\n",
      "Epoch: 1, Step: 78, loss: 0.9754\n",
      "LR: 0.00032\n",
      "Epoch: 1, Step: 79, loss: 0.9798\n",
      "LR: 0.000324\n",
      "Epoch: 1, Step: 80, loss: 0.9814\n",
      "LR: 0.000328\n",
      "Epoch: 1, Step: 81, loss: 0.9775\n",
      "LR: 0.00033200000000000005\n",
      "Epoch: 1, Step: 82, loss: 0.9722\n",
      "LR: 0.00033600000000000004\n",
      "Epoch: 1, Step: 83, loss: 0.9771\n",
      "LR: 0.00034\n",
      "Epoch: 1, Step: 84, loss: 0.9770\n",
      "LR: 0.00034399999999999996\n",
      "Epoch: 1, Step: 85, loss: 0.9806\n",
      "LR: 0.000348\n",
      "Epoch: 1, Step: 86, loss: 0.9765\n",
      "LR: 0.000352\n",
      "Epoch: 1, Step: 87, loss: 0.9787\n",
      "LR: 0.000356\n",
      "Epoch: 1, Step: 88, loss: 0.9759\n",
      "LR: 0.00035999999999999997\n",
      "Epoch: 1, Step: 89, loss: 0.9802\n",
      "LR: 0.000364\n",
      "Epoch: 1, Step: 90, loss: 0.9825\n",
      "LR: 0.000368\n",
      "Epoch: 1, Step: 91, loss: 0.9853\n",
      "Epoch: 001, loss_train: 1.3565, loss_valid:1.6538, AUC:0.8029, ap: 0.7673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.000372\n",
      "Epoch: 2, Step: 0, loss: 0.9886\n",
      "LR: 0.00037600000000000003\n",
      "Epoch: 2, Step: 1, loss: 0.9819\n",
      "LR: 0.00038\n",
      "Epoch: 2, Step: 2, loss: 0.9827\n",
      "LR: 0.000384\n",
      "Epoch: 2, Step: 3, loss: 0.9730\n",
      "LR: 0.000388\n",
      "Epoch: 2, Step: 4, loss: 0.9806\n",
      "LR: 0.00039200000000000004\n",
      "Epoch: 2, Step: 5, loss: 0.9733\n",
      "LR: 0.00039600000000000003\n",
      "Epoch: 2, Step: 6, loss: 0.9748\n",
      "LR: 0.0004\n",
      "Epoch: 2, Step: 7, loss: 0.9821\n",
      "LR: 0.000404\n",
      "Epoch: 2, Step: 8, loss: 0.9806\n",
      "LR: 0.000408\n",
      "Epoch: 2, Step: 9, loss: 0.9753\n",
      "LR: 0.000412\n",
      "Epoch: 2, Step: 10, loss: 0.9794\n",
      "LR: 0.000416\n",
      "Epoch: 2, Step: 11, loss: 0.9774\n",
      "LR: 0.00042\n",
      "Epoch: 2, Step: 12, loss: 0.9798\n",
      "LR: 0.000424\n",
      "Epoch: 2, Step: 13, loss: 0.9757\n",
      "LR: 0.000428\n",
      "Epoch: 2, Step: 14, loss: 0.9813\n",
      "LR: 0.000432\n",
      "Epoch: 2, Step: 15, loss: 0.9730\n",
      "LR: 0.000436\n",
      "Epoch: 2, Step: 16, loss: 0.9797\n",
      "LR: 0.00044\n",
      "Epoch: 2, Step: 17, loss: 0.9719\n",
      "LR: 0.000444\n",
      "Epoch: 2, Step: 18, loss: 0.9800\n",
      "LR: 0.000448\n",
      "Epoch: 2, Step: 19, loss: 0.9705\n",
      "LR: 0.00045200000000000004\n",
      "Epoch: 2, Step: 20, loss: 0.9722\n",
      "LR: 0.000456\n",
      "Epoch: 2, Step: 21, loss: 0.9742\n",
      "LR: 0.00046\n",
      "Epoch: 2, Step: 22, loss: 0.9785\n",
      "LR: 0.00046400000000000006\n",
      "Epoch: 2, Step: 23, loss: 0.9773\n",
      "LR: 0.00046800000000000005\n",
      "Epoch: 2, Step: 24, loss: 0.9836\n",
      "LR: 0.000472\n",
      "Epoch: 2, Step: 25, loss: 0.9811\n",
      "LR: 0.00047599999999999997\n",
      "Epoch: 2, Step: 26, loss: 0.9760\n",
      "LR: 0.00048\n",
      "Epoch: 2, Step: 27, loss: 0.9782\n",
      "LR: 0.000484\n",
      "Epoch: 2, Step: 28, loss: 0.9769\n",
      "LR: 0.000488\n",
      "Epoch: 2, Step: 29, loss: 0.9823\n",
      "LR: 0.000492\n",
      "Epoch: 2, Step: 30, loss: 0.9745\n",
      "LR: 0.000496\n",
      "Epoch: 2, Step: 31, loss: 0.9761\n",
      "LR: 0.0005\n",
      "Epoch: 2, Step: 32, loss: 0.9778\n",
      "LR: 0.000504\n",
      "Epoch: 2, Step: 33, loss: 0.9739\n",
      "LR: 0.000508\n",
      "Epoch: 2, Step: 34, loss: 0.9822\n",
      "LR: 0.000512\n",
      "Epoch: 2, Step: 35, loss: 0.9746\n",
      "LR: 0.0005160000000000001\n",
      "Epoch: 2, Step: 36, loss: 0.9754\n",
      "LR: 0.0005200000000000001\n",
      "Epoch: 2, Step: 37, loss: 0.9802\n",
      "LR: 0.000524\n",
      "Epoch: 2, Step: 38, loss: 0.9751\n",
      "LR: 0.000528\n",
      "Epoch: 2, Step: 39, loss: 0.9765\n",
      "LR: 0.000532\n",
      "Epoch: 2, Step: 40, loss: 0.9760\n",
      "LR: 0.000536\n",
      "Epoch: 2, Step: 41, loss: 0.9745\n",
      "LR: 0.00054\n",
      "Epoch: 2, Step: 42, loss: 0.9717\n",
      "LR: 0.0005440000000000001\n",
      "Epoch: 2, Step: 43, loss: 0.9741\n",
      "LR: 0.0005480000000000001\n",
      "Epoch: 2, Step: 44, loss: 0.9786\n",
      "LR: 0.0005520000000000001\n",
      "Epoch: 2, Step: 45, loss: 0.9741\n",
      "LR: 0.0005560000000000001\n",
      "Epoch: 2, Step: 46, loss: 0.9710\n",
      "LR: 0.0005600000000000001\n",
      "Epoch: 2, Step: 47, loss: 0.9818\n",
      "LR: 0.0005639999999999999\n",
      "Epoch: 2, Step: 48, loss: 0.9730\n",
      "LR: 0.0005679999999999999\n",
      "Epoch: 2, Step: 49, loss: 0.9739\n",
      "LR: 0.0005719999999999999\n",
      "Epoch: 2, Step: 50, loss: 0.9742\n",
      "LR: 0.000576\n",
      "Epoch: 2, Step: 51, loss: 0.9714\n",
      "LR: 0.00058\n",
      "Epoch: 2, Step: 52, loss: 0.9774\n",
      "LR: 0.000584\n",
      "Epoch: 2, Step: 53, loss: 0.9754\n",
      "LR: 0.000588\n",
      "Epoch: 2, Step: 54, loss: 0.9811\n",
      "LR: 0.000592\n",
      "Epoch: 2, Step: 55, loss: 0.9723\n",
      "LR: 0.000596\n",
      "Epoch: 2, Step: 56, loss: 0.9749\n",
      "LR: 0.0006\n",
      "Epoch: 2, Step: 57, loss: 0.9716\n",
      "LR: 0.000604\n",
      "Epoch: 2, Step: 58, loss: 0.9766\n",
      "LR: 0.000608\n",
      "Epoch: 2, Step: 59, loss: 0.9756\n",
      "LR: 0.000612\n",
      "Epoch: 2, Step: 60, loss: 0.9771\n",
      "LR: 0.000616\n",
      "Epoch: 2, Step: 61, loss: 0.9738\n",
      "LR: 0.00062\n",
      "Epoch: 2, Step: 62, loss: 0.9743\n",
      "LR: 0.000624\n",
      "Epoch: 2, Step: 63, loss: 0.9755\n",
      "LR: 0.000628\n",
      "Epoch: 2, Step: 64, loss: 0.9731\n",
      "LR: 0.000632\n",
      "Epoch: 2, Step: 65, loss: 0.9749\n",
      "LR: 0.0006360000000000001\n",
      "Epoch: 2, Step: 66, loss: 0.9732\n",
      "LR: 0.00064\n",
      "Epoch: 2, Step: 67, loss: 0.9739\n",
      "LR: 0.000644\n",
      "Epoch: 2, Step: 68, loss: 0.9677\n",
      "LR: 0.000648\n",
      "Epoch: 2, Step: 69, loss: 0.9773\n",
      "LR: 0.000652\n",
      "Epoch: 2, Step: 70, loss: 0.9738\n",
      "LR: 0.000656\n",
      "Epoch: 2, Step: 71, loss: 0.9745\n",
      "LR: 0.00066\n",
      "Epoch: 2, Step: 72, loss: 0.9721\n",
      "LR: 0.0006640000000000001\n",
      "Epoch: 2, Step: 73, loss: 0.9690\n",
      "LR: 0.0006680000000000001\n",
      "Epoch: 2, Step: 74, loss: 0.9718\n",
      "LR: 0.0006720000000000001\n",
      "Epoch: 2, Step: 75, loss: 0.9703\n",
      "LR: 0.0006760000000000001\n",
      "Epoch: 2, Step: 76, loss: 0.9775\n",
      "LR: 0.00068\n",
      "Epoch: 2, Step: 77, loss: 0.9811\n",
      "LR: 0.000684\n",
      "Epoch: 2, Step: 78, loss: 0.9809\n",
      "LR: 0.0006879999999999999\n",
      "Epoch: 2, Step: 79, loss: 0.9747\n",
      "LR: 0.000692\n",
      "Epoch: 2, Step: 80, loss: 0.9769\n",
      "LR: 0.000696\n",
      "Epoch: 2, Step: 81, loss: 0.9741\n",
      "LR: 0.0007\n",
      "Epoch: 2, Step: 82, loss: 0.9726\n",
      "LR: 0.000704\n",
      "Epoch: 2, Step: 83, loss: 0.9734\n",
      "LR: 0.000708\n",
      "Epoch: 2, Step: 84, loss: 0.9741\n",
      "LR: 0.000712\n",
      "Epoch: 2, Step: 85, loss: 0.9752\n",
      "LR: 0.000716\n",
      "Epoch: 2, Step: 86, loss: 0.9775\n",
      "LR: 0.0007199999999999999\n",
      "Epoch: 2, Step: 87, loss: 0.9775\n",
      "LR: 0.000724\n",
      "Epoch: 2, Step: 88, loss: 0.9727\n",
      "LR: 0.000728\n",
      "Epoch: 2, Step: 89, loss: 0.9710\n",
      "LR: 0.000732\n",
      "Epoch: 2, Step: 90, loss: 0.9747\n",
      "LR: 0.000736\n",
      "Epoch: 2, Step: 91, loss: 0.9770\n",
      "Epoch: 002, loss_train: 0.9760, loss_valid:1.6272, AUC:0.7836, ap: 0.7365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.00074\n",
      "Epoch: 3, Step: 0, loss: 0.9769\n",
      "LR: 0.000744\n",
      "Epoch: 3, Step: 1, loss: 0.9772\n",
      "LR: 0.000748\n",
      "Epoch: 3, Step: 2, loss: 0.9741\n",
      "LR: 0.0007520000000000001\n",
      "Epoch: 3, Step: 3, loss: 0.9755\n",
      "LR: 0.000756\n",
      "Epoch: 3, Step: 4, loss: 0.9747\n",
      "LR: 0.00076\n",
      "Epoch: 3, Step: 5, loss: 0.9732\n",
      "LR: 0.000764\n",
      "Epoch: 3, Step: 6, loss: 0.9739\n",
      "LR: 0.000768\n",
      "Epoch: 3, Step: 7, loss: 0.9715\n",
      "LR: 0.000772\n",
      "Epoch: 3, Step: 8, loss: 0.9787\n",
      "LR: 0.000776\n",
      "Epoch: 3, Step: 9, loss: 0.9704\n",
      "LR: 0.0007800000000000001\n",
      "Epoch: 3, Step: 10, loss: 0.9749\n",
      "LR: 0.0007840000000000001\n",
      "Epoch: 3, Step: 11, loss: 0.9763\n",
      "LR: 0.0007880000000000001\n",
      "Epoch: 3, Step: 12, loss: 0.9710\n",
      "LR: 0.0007920000000000001\n",
      "Epoch: 3, Step: 13, loss: 0.9752\n",
      "LR: 0.000796\n",
      "Epoch: 3, Step: 14, loss: 0.9712\n",
      "LR: 0.0008\n",
      "Epoch: 3, Step: 15, loss: 0.9752\n",
      "LR: 0.000804\n",
      "Epoch: 3, Step: 16, loss: 0.9797\n",
      "LR: 0.000808\n",
      "Epoch: 3, Step: 17, loss: 0.9685\n",
      "LR: 0.0008120000000000001\n",
      "Epoch: 3, Step: 18, loss: 0.9782\n",
      "LR: 0.000816\n",
      "Epoch: 3, Step: 19, loss: 0.9747\n",
      "LR: 0.00082\n",
      "Epoch: 3, Step: 20, loss: 0.9769\n",
      "LR: 0.000824\n",
      "Epoch: 3, Step: 21, loss: 0.9803\n",
      "LR: 0.000828\n",
      "Epoch: 3, Step: 22, loss: 0.9782\n",
      "LR: 0.000832\n",
      "Epoch: 3, Step: 23, loss: 0.9805\n",
      "LR: 0.0008359999999999999\n",
      "Epoch: 3, Step: 24, loss: 0.9748\n",
      "LR: 0.00084\n",
      "Epoch: 3, Step: 25, loss: 0.9735\n",
      "LR: 0.000844\n",
      "Epoch: 3, Step: 26, loss: 0.9749\n",
      "LR: 0.000848\n",
      "Epoch: 3, Step: 27, loss: 0.9755\n",
      "LR: 0.000852\n",
      "Epoch: 3, Step: 28, loss: 0.9755\n",
      "LR: 0.000856\n",
      "Epoch: 3, Step: 29, loss: 0.9769\n",
      "LR: 0.00086\n",
      "Epoch: 3, Step: 30, loss: 0.9796\n",
      "LR: 0.000864\n",
      "Epoch: 3, Step: 31, loss: 0.9747\n",
      "LR: 0.0008680000000000001\n",
      "Epoch: 3, Step: 32, loss: 0.9809\n",
      "LR: 0.000872\n",
      "Epoch: 3, Step: 33, loss: 0.9726\n",
      "LR: 0.000876\n",
      "Epoch: 3, Step: 34, loss: 0.9800\n",
      "LR: 0.00088\n",
      "Epoch: 3, Step: 35, loss: 0.9788\n",
      "LR: 0.000884\n",
      "Epoch: 3, Step: 36, loss: 0.9735\n",
      "LR: 0.000888\n",
      "Epoch: 3, Step: 37, loss: 0.9755\n",
      "LR: 0.000892\n",
      "Epoch: 3, Step: 38, loss: 0.9746\n",
      "LR: 0.000896\n",
      "Epoch: 3, Step: 39, loss: 0.9752\n",
      "LR: 0.0009000000000000001\n",
      "Epoch: 3, Step: 40, loss: 0.9745\n",
      "LR: 0.0009040000000000001\n",
      "Epoch: 3, Step: 41, loss: 0.9727\n",
      "LR: 0.0009080000000000001\n",
      "Epoch: 3, Step: 42, loss: 0.9774\n",
      "LR: 0.000912\n",
      "Epoch: 3, Step: 43, loss: 0.9666\n",
      "LR: 0.000916\n",
      "Epoch: 3, Step: 44, loss: 0.9769\n",
      "LR: 0.00092\n",
      "Epoch: 3, Step: 45, loss: 0.9813\n",
      "LR: 0.000924\n",
      "Epoch: 3, Step: 46, loss: 0.9759\n",
      "LR: 0.0009280000000000001\n",
      "Epoch: 3, Step: 47, loss: 0.9750\n",
      "LR: 0.0009320000000000001\n",
      "Epoch: 3, Step: 48, loss: 0.9733\n",
      "LR: 0.0009360000000000001\n",
      "Epoch: 3, Step: 49, loss: 0.9762\n",
      "LR: 0.00094\n",
      "Epoch: 3, Step: 50, loss: 0.9738\n",
      "LR: 0.000944\n",
      "Epoch: 3, Step: 51, loss: 0.9725\n",
      "LR: 0.000948\n",
      "Epoch: 3, Step: 52, loss: 0.9761\n",
      "LR: 0.0009519999999999999\n",
      "Epoch: 3, Step: 53, loss: 0.9772\n",
      "LR: 0.0009559999999999999\n",
      "Epoch: 3, Step: 54, loss: 0.9728\n",
      "LR: 0.00096\n",
      "Epoch: 3, Step: 55, loss: 0.9780\n",
      "LR: 0.000964\n",
      "Epoch: 3, Step: 56, loss: 0.9714\n",
      "LR: 0.000968\n",
      "Epoch: 3, Step: 57, loss: 0.9721\n",
      "LR: 0.000972\n",
      "Epoch: 3, Step: 58, loss: 0.9770\n",
      "LR: 0.000976\n",
      "Epoch: 3, Step: 59, loss: 0.9737\n",
      "LR: 0.00098\n",
      "Epoch: 3, Step: 60, loss: 0.9778\n",
      "LR: 0.000984\n",
      "Epoch: 3, Step: 61, loss: 0.9737\n",
      "LR: 0.000988\n",
      "Epoch: 3, Step: 62, loss: 0.9799\n",
      "LR: 0.000992\n",
      "Epoch: 3, Step: 63, loss: 0.9692\n",
      "LR: 0.000996\n",
      "Epoch: 3, Step: 64, loss: 0.9704\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 65, loss: 0.9734\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 66, loss: 0.9704\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 67, loss: 0.9773\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 68, loss: 0.9737\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 69, loss: 0.9688\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 70, loss: 0.9710\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 71, loss: 0.9732\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 72, loss: 0.9754\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 73, loss: 0.9693\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 74, loss: 0.9835\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 75, loss: 0.9760\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 76, loss: 0.9747\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 77, loss: 0.9706\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 78, loss: 0.9810\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 79, loss: 0.9734\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 80, loss: 0.9819\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 81, loss: 0.9787\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 82, loss: 0.9746\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 83, loss: 0.9741\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 84, loss: 0.9782\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 85, loss: 0.9777\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 86, loss: 0.9723\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 87, loss: 0.9711\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 88, loss: 0.9765\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 89, loss: 0.9793\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 90, loss: 0.9757\n",
      "LR: 0.001\n",
      "Epoch: 3, Step: 91, loss: 0.9735\n",
      "Epoch: 003, loss_train: 0.9752, loss_valid:1.8024, AUC:0.7800, ap: 0.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.001\n",
      "Epoch: 4, Step: 0, loss: 0.9775\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 1, loss: 0.9788\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 2, loss: 0.9813\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 3, loss: 0.9794\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 4, loss: 0.9699\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 5, loss: 0.9748\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 6, loss: 0.9635\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 7, loss: 0.9812\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 8, loss: 0.9794\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 9, loss: 0.9703\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 10, loss: 0.9762\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 11, loss: 0.9698\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 12, loss: 0.9761\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 13, loss: 0.9669\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 14, loss: 0.9719\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 15, loss: 0.9678\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 16, loss: 0.9736\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 17, loss: 0.9734\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 18, loss: 0.9747\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 19, loss: 0.9762\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 20, loss: 0.9741\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 21, loss: 0.9790\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 22, loss: 0.9726\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 23, loss: 0.9727\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 24, loss: 0.9720\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 25, loss: 0.9719\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 26, loss: 0.9789\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 27, loss: 0.9684\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 28, loss: 0.9761\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 29, loss: 0.9764\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 30, loss: 0.9699\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 31, loss: 0.9737\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 32, loss: 0.9709\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 33, loss: 0.9754\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 34, loss: 0.9678\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 35, loss: 0.9732\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 36, loss: 0.9691\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 37, loss: 0.9733\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 38, loss: 0.9791\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 39, loss: 0.9736\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 40, loss: 0.9745\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 41, loss: 0.9743\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 42, loss: 0.9735\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 43, loss: 0.9720\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 44, loss: 0.9763\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 45, loss: 0.9741\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 46, loss: 0.9773\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 47, loss: 0.9786\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 48, loss: 0.9752\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 49, loss: 0.9787\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 50, loss: 0.9747\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 51, loss: 0.9825\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 52, loss: 0.9697\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 53, loss: 0.9667\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 54, loss: 0.9786\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 55, loss: 0.9764\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 56, loss: 0.9720\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 57, loss: 0.9754\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 58, loss: 0.9730\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 59, loss: 0.9735\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 60, loss: 0.9762\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 61, loss: 0.9763\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 62, loss: 0.9692\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 63, loss: 0.9755\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 64, loss: 0.9691\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 65, loss: 0.9729\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 66, loss: 0.9725\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 67, loss: 0.9672\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 68, loss: 0.9764\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 69, loss: 0.9791\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 70, loss: 0.9701\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 71, loss: 0.9731\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 72, loss: 0.9707\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 73, loss: 0.9778\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 74, loss: 0.9790\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 75, loss: 0.9735\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 76, loss: 0.9680\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 77, loss: 0.9757\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 78, loss: 0.9727\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 79, loss: 0.9705\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 80, loss: 0.9705\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 81, loss: 0.9700\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 82, loss: 0.9699\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 83, loss: 0.9743\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 84, loss: 0.9836\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 85, loss: 0.9763\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 86, loss: 0.9741\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 87, loss: 0.9742\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 88, loss: 0.9735\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 89, loss: 0.9816\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 90, loss: 0.9691\n",
      "LR: 0.001\n",
      "Epoch: 4, Step: 91, loss: 0.9669\n",
      "Epoch: 004, loss_train: 0.9739, loss_valid:1.8228, AUC:0.7770, ap: 0.7021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.001\n",
      "Epoch: 5, Step: 0, loss: 0.9839\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 1, loss: 0.9741\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 2, loss: 0.9764\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 3, loss: 0.9789\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 4, loss: 0.9858\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 5, loss: 0.9793\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 6, loss: 0.9787\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 7, loss: 0.9761\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 8, loss: 0.9732\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 9, loss: 0.9803\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 10, loss: 0.9719\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 11, loss: 0.9716\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 12, loss: 0.9707\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 13, loss: 0.9719\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 14, loss: 0.9729\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 15, loss: 0.9684\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 16, loss: 0.9692\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 17, loss: 0.9738\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 18, loss: 0.9653\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 19, loss: 0.9741\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 20, loss: 0.9695\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 21, loss: 0.9679\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 22, loss: 0.9714\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 23, loss: 0.9722\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 24, loss: 0.9665\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 25, loss: 0.9708\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 26, loss: 0.9740\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 27, loss: 0.9764\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 28, loss: 0.9787\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 29, loss: 0.9718\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 30, loss: 0.9794\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 31, loss: 0.9662\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 32, loss: 0.9709\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 33, loss: 0.9753\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 34, loss: 0.9754\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 35, loss: 0.9762\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 36, loss: 0.9672\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 37, loss: 0.9700\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 38, loss: 0.9704\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 39, loss: 0.9755\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 40, loss: 0.9756\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 41, loss: 0.9696\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 42, loss: 0.9738\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 43, loss: 0.9659\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 44, loss: 0.9798\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 45, loss: 0.9737\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 46, loss: 0.9738\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 47, loss: 0.9714\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 48, loss: 0.9738\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 49, loss: 0.9733\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 50, loss: 0.9738\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 51, loss: 0.9750\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 52, loss: 0.9703\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 53, loss: 0.9703\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 54, loss: 0.9727\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 55, loss: 0.9702\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 56, loss: 0.9749\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 57, loss: 0.9782\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 58, loss: 0.9763\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 59, loss: 0.9702\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 60, loss: 0.9775\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 61, loss: 0.9693\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 62, loss: 0.9704\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 63, loss: 0.9755\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 64, loss: 0.9743\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 65, loss: 0.9659\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 66, loss: 0.9717\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 67, loss: 0.9702\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 68, loss: 0.9722\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 69, loss: 0.9668\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 70, loss: 0.9688\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 71, loss: 0.9757\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 72, loss: 0.9667\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 73, loss: 0.9793\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 74, loss: 0.9679\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 75, loss: 0.9719\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 76, loss: 0.9743\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 77, loss: 0.9686\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 78, loss: 0.9747\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 79, loss: 0.9720\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 80, loss: 0.9717\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 81, loss: 0.9733\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 82, loss: 0.9796\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 83, loss: 0.9699\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 84, loss: 0.9724\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 85, loss: 0.9729\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 86, loss: 0.9752\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 87, loss: 0.9705\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 88, loss: 0.9716\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 89, loss: 0.9740\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 90, loss: 0.9742\n",
      "LR: 0.001\n",
      "Epoch: 5, Step: 91, loss: 0.9778\n",
      "Epoch: 005, loss_train: 0.9731, loss_valid:1.8176, AUC:0.7936, ap: 0.7442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.001\n",
      "Epoch: 6, Step: 0, loss: 0.9759\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 1, loss: 0.9777\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 2, loss: 0.9713\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 3, loss: 0.9769\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 4, loss: 0.9696\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 5, loss: 0.9712\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 6, loss: 0.9767\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 7, loss: 0.9745\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 8, loss: 0.9678\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 9, loss: 0.9707\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 10, loss: 0.9732\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 11, loss: 0.9736\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 12, loss: 0.9738\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 13, loss: 0.9686\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 14, loss: 0.9717\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 15, loss: 0.9681\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 16, loss: 0.9685\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 17, loss: 0.9675\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 18, loss: 0.9743\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 19, loss: 0.9726\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 20, loss: 0.9673\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 21, loss: 0.9720\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 22, loss: 0.9755\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 23, loss: 0.9706\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 24, loss: 0.9695\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 25, loss: 0.9789\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 26, loss: 0.9759\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 27, loss: 0.9755\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 28, loss: 0.9698\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 29, loss: 0.9717\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 30, loss: 0.9752\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 31, loss: 0.9690\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 32, loss: 0.9703\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 33, loss: 0.9753\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 34, loss: 0.9714\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 35, loss: 0.9700\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 36, loss: 0.9708\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 37, loss: 0.9759\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 38, loss: 0.9769\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 39, loss: 0.9683\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 40, loss: 0.9652\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 41, loss: 0.9805\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 42, loss: 0.9752\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 43, loss: 0.9720\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 44, loss: 0.9709\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 45, loss: 0.9798\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 46, loss: 0.9780\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 47, loss: 0.9742\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 48, loss: 0.9735\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 49, loss: 0.9661\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 50, loss: 0.9757\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 51, loss: 0.9790\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 52, loss: 0.9728\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 53, loss: 0.9719\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 54, loss: 0.9747\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 55, loss: 0.9733\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 56, loss: 0.9726\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 57, loss: 0.9749\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 58, loss: 0.9656\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 59, loss: 0.9718\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 60, loss: 0.9755\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 61, loss: 0.9707\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 62, loss: 0.9673\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 63, loss: 0.9747\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 64, loss: 0.9648\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 65, loss: 0.9696\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 66, loss: 0.9745\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 67, loss: 0.9704\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 68, loss: 0.9675\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 69, loss: 0.9749\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 70, loss: 0.9653\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 71, loss: 0.9727\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 72, loss: 0.9660\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 73, loss: 0.9672\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 74, loss: 0.9692\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 75, loss: 0.9707\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 76, loss: 0.9750\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 77, loss: 0.9707\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 78, loss: 0.9736\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 79, loss: 0.9707\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 80, loss: 0.9719\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 81, loss: 0.9692\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 82, loss: 0.9722\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 83, loss: 0.9679\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 84, loss: 0.9685\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 85, loss: 0.9713\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 86, loss: 0.9701\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 87, loss: 0.9686\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 88, loss: 0.9698\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 89, loss: 0.9722\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 90, loss: 0.9688\n",
      "LR: 0.001\n",
      "Epoch: 6, Step: 91, loss: 0.9677\n",
      "Epoch: 006, loss_train: 0.9719, loss_valid:1.8592, AUC:0.7881, ap: 0.7296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.001\n",
      "Epoch: 7, Step: 0, loss: 0.9657\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 1, loss: 0.9663\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 2, loss: 0.9707\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 3, loss: 0.9742\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 4, loss: 0.9693\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 5, loss: 0.9700\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 6, loss: 0.9783\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 7, loss: 0.9679\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 8, loss: 0.9686\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 9, loss: 0.9683\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 10, loss: 0.9711\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 11, loss: 0.9724\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 12, loss: 0.9743\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 13, loss: 0.9698\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 14, loss: 0.9680\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 15, loss: 0.9770\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 16, loss: 0.9696\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 17, loss: 0.9719\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 18, loss: 0.9729\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 19, loss: 0.9685\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 20, loss: 0.9692\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 21, loss: 0.9719\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 22, loss: 0.9690\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 23, loss: 0.9706\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 24, loss: 0.9817\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 25, loss: 0.9663\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 26, loss: 0.9706\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 27, loss: 0.9683\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 28, loss: 0.9665\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 29, loss: 0.9697\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 30, loss: 0.9748\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 31, loss: 0.9720\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 32, loss: 0.9700\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 33, loss: 0.9705\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 34, loss: 0.9665\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 35, loss: 0.9718\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 36, loss: 0.9706\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 37, loss: 0.9662\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 38, loss: 0.9732\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 39, loss: 0.9645\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 40, loss: 0.9719\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 41, loss: 0.9662\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 42, loss: 0.9713\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 43, loss: 0.9754\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 44, loss: 0.9674\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 45, loss: 0.9697\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 46, loss: 0.9699\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 47, loss: 0.9763\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 48, loss: 0.9735\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 49, loss: 0.9777\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 50, loss: 0.9767\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 51, loss: 0.9632\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 52, loss: 0.9696\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 53, loss: 0.9671\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 54, loss: 0.9737\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 55, loss: 0.9710\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 56, loss: 0.9706\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 57, loss: 0.9712\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 58, loss: 0.9726\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 59, loss: 0.9747\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 60, loss: 0.9672\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 61, loss: 0.9728\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 62, loss: 0.9722\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 63, loss: 0.9726\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 64, loss: 0.9741\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 65, loss: 0.9666\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 66, loss: 0.9775\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 67, loss: 0.9703\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 68, loss: 0.9780\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 69, loss: 0.9755\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 70, loss: 0.9658\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 71, loss: 0.9700\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 72, loss: 0.9679\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 73, loss: 0.9719\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 74, loss: 0.9738\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 75, loss: 0.9731\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 76, loss: 0.9702\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 77, loss: 0.9734\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 78, loss: 0.9680\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 79, loss: 0.9731\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 80, loss: 0.9681\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 81, loss: 0.9693\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 82, loss: 0.9708\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 83, loss: 0.9715\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 84, loss: 0.9741\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 85, loss: 0.9713\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 86, loss: 0.9675\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 87, loss: 0.9786\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 88, loss: 0.9628\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 89, loss: 0.9738\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 90, loss: 0.9734\n",
      "LR: 0.001\n",
      "Epoch: 7, Step: 91, loss: 0.9714\n",
      "Early stopping at epoch 7\n"
     ]
    }
   ],
   "source": [
    "#loop\n",
    "log_training_parameters(batch_size, learning_rate, weight_decay, epochs, neighbors_per_node, num_layers, embedding_dim, activation, normalization)\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "best_loss = 100000\n",
    "for epoch in range (1, epochs +1):\n",
    "  if(model_name == \"VGAE\"):\n",
    "    loss = train_VGAE()\n",
    "  elif(model_name == \"GAE\"):\n",
    "    loss = train()\n",
    "  auc, ap, loss_valid = valid(valid_pos_edge_index, valid_neg_edge_index)\n",
    "  if (loss_valid < best_loss):\n",
    "    best_loss = loss_valid\n",
    "    epochs_without_improvement = 0\n",
    "    best_model = model.state_dict()\n",
    "  else:\n",
    "    epochs_without_improvement += 1\n",
    "    if (epochs_without_improvement == patience):\n",
    "      print (f'Early stopping at epoch {epoch}')\n",
    "      break\n",
    "  log_epoch_performance(epoch, loss, loss_valid, auc, ap, epochs, embedding_dim, learning_rate, weight_decay, neighbors_per_node, num_layers, normalization)\n",
    "  writer.add_scalar('auc valid',auc,epoch)\n",
    "  writer.add_scalar('ap valid',ap,epoch)\n",
    "  writer.add_scalar('loss valid',loss_valid,epoch)\n",
    "  writer.add_scalar('loss train',loss, epoch)\n",
    "  print (f'Epoch: {epoch:03d}, loss_train: {loss:.4f}, loss_valid:{loss_valid:.4f}, AUC:{auc:.4f}, ap: {ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ewRIiiqC4w-",
    "outputId": "48c6ab7d-1777-4390-8ed4-26e4db37d44a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step: 0, LR: 0.000004, loss: 3.1616\n",
      "Epoch: 1, Step: 1, LR: 0.000008, loss: 3.1504\n",
      "Epoch: 1, Step: 2, LR: 0.000012, loss: 3.0851\n",
      "Epoch: 1, Step: 3, LR: 0.000016, loss: 2.9635\n",
      "Epoch: 1, Step: 4, LR: 0.000020, loss: 2.7958\n",
      "Epoch: 1, Step: 5, LR: 0.000024, loss: 2.5076\n",
      "Epoch: 1, Step: 6, LR: 0.000028, loss: 2.2629\n",
      "Epoch: 1, Step: 7, LR: 0.000032, loss: 1.9707\n",
      "Epoch: 1, Step: 8, LR: 0.000036, loss: 1.7233\n",
      "Epoch: 1, Step: 9, LR: 0.000040, loss: 1.5702\n",
      "Epoch: 1, Step: 10, LR: 0.000044, loss: 1.5783\n",
      "Epoch: 1, Step: 11, LR: 0.000048, loss: 1.7014\n",
      "Epoch: 1, Step: 12, LR: 0.000052, loss: 1.8662\n",
      "Epoch: 1, Step: 13, LR: 0.000056, loss: 1.9824\n",
      "Epoch: 1, Step: 14, LR: 0.000060, loss: 1.9637\n",
      "Epoch: 1, Step: 15, LR: 0.000064, loss: 1.8867\n",
      "Epoch: 1, Step: 16, LR: 0.000068, loss: 1.6414\n",
      "Epoch: 1, Step: 17, LR: 0.000072, loss: 1.4250\n",
      "Epoch: 1, Step: 18, LR: 0.000076, loss: 1.1951\n",
      "Epoch: 1, Step: 19, LR: 0.000080, loss: 1.0411\n",
      "Epoch: 1, Step: 20, LR: 0.000084, loss: 0.9992\n",
      "Epoch: 1, Step: 21, LR: 0.000088, loss: 1.0596\n",
      "Epoch: 1, Step: 22, LR: 0.000092, loss: 1.1465\n",
      "Epoch: 1, Step: 23, LR: 0.000096, loss: 1.1240\n",
      "Epoch: 1, Step: 24, LR: 0.000100, loss: 1.0743\n",
      "Epoch: 1, Step: 25, LR: 0.000104, loss: 0.9706\n",
      "Epoch: 1, Step: 26, LR: 0.000108, loss: 0.8043\n",
      "Epoch: 1, Step: 27, LR: 0.000112, loss: 0.7389\n",
      "Epoch: 1, Step: 28, LR: 0.000116, loss: 0.7426\n",
      "Epoch: 1, Step: 29, LR: 0.000120, loss: 0.7286\n",
      "Epoch: 1, Step: 30, LR: 0.000124, loss: 0.7828\n",
      "Epoch: 1, Step: 31, LR: 0.000128, loss: 0.7533\n",
      "Epoch: 1, Step: 32, LR: 0.000132, loss: 0.6975\n",
      "Epoch: 1, Step: 33, LR: 0.000136, loss: 0.6390\n",
      "Epoch: 1, Step: 34, LR: 0.000140, loss: 0.5751\n",
      "Epoch: 1, Step: 35, LR: 0.000144, loss: 0.5671\n",
      "Epoch: 1, Step: 36, LR: 0.000148, loss: 0.5620\n",
      "Epoch: 1, Step: 37, LR: 0.000152, loss: 0.5875\n",
      "Epoch: 1, Step: 38, LR: 0.000156, loss: 0.5563\n",
      "Epoch: 1, Step: 39, LR: 0.000160, loss: 0.5612\n",
      "Epoch: 1, Step: 40, LR: 0.000164, loss: 0.5339\n",
      "Epoch: 1, Step: 41, LR: 0.000168, loss: 0.4736\n",
      "Epoch: 1, Step: 42, LR: 0.000172, loss: 0.4794\n",
      "Epoch: 1, Step: 43, LR: 0.000176, loss: 0.4510\n",
      "Epoch: 1, Step: 44, LR: 0.000180, loss: 0.4396\n",
      "Epoch: 1, Step: 45, LR: 0.000184, loss: 0.4450\n",
      "Epoch: 1, Step: 46, LR: 0.000188, loss: 0.4160\n",
      "Epoch: 1, Step: 47, LR: 0.000192, loss: 0.4054\n",
      "Epoch: 1, Step: 48, LR: 0.000196, loss: 0.3898\n",
      "Epoch: 1, Step: 49, LR: 0.000200, loss: 0.4137\n",
      "Epoch: 1, Step: 50, LR: 0.000204, loss: 0.3717\n",
      "Epoch: 1, Step: 51, LR: 0.000208, loss: 0.3560\n",
      "Epoch: 1, Step: 52, LR: 0.000212, loss: 0.3451\n",
      "Epoch: 1, Step: 53, LR: 0.000216, loss: 0.3224\n",
      "Epoch: 1, Step: 54, LR: 0.000220, loss: 0.3362\n",
      "Epoch: 1, Step: 55, LR: 0.000224, loss: 0.3515\n",
      "Epoch: 1, Step: 56, LR: 0.000228, loss: 0.3059\n",
      "Epoch: 1, Step: 57, LR: 0.000232, loss: 0.3061\n",
      "Epoch: 1, Step: 58, LR: 0.000236, loss: 0.2938\n",
      "Epoch: 1, Step: 59, LR: 0.000240, loss: 0.2793\n",
      "Epoch: 1, Step: 60, LR: 0.000244, loss: 0.2737\n",
      "Epoch: 1, Step: 61, LR: 0.000248, loss: 0.2573\n",
      "Epoch: 1, Step: 62, LR: 0.000252, loss: 0.2692\n",
      "Epoch: 1, Step: 63, LR: 0.000256, loss: 0.2430\n",
      "Epoch: 1, Step: 64, LR: 0.000260, loss: 0.2357\n",
      "Epoch: 1, Step: 65, LR: 0.000264, loss: 0.2286\n",
      "Epoch: 1, Step: 66, LR: 0.000268, loss: 0.2304\n",
      "Epoch: 1, Step: 67, LR: 0.000272, loss: 0.2083\n",
      "Epoch: 1, Step: 68, LR: 0.000276, loss: 0.2154\n",
      "Epoch: 1, Step: 69, LR: 0.000280, loss: 0.1956\n",
      "Epoch: 1, Step: 70, LR: 0.000284, loss: 0.1993\n",
      "Epoch: 1, Step: 71, LR: 0.000288, loss: 0.1944\n",
      "Epoch: 1, Step: 72, LR: 0.000292, loss: 0.1806\n",
      "Epoch: 1, Step: 73, LR: 0.000296, loss: 0.1870\n",
      "Epoch: 1, Step: 74, LR: 0.000300, loss: 0.1657\n",
      "Epoch: 1, Step: 75, LR: 0.000304, loss: 0.1533\n",
      "Epoch: 1, Step: 76, LR: 0.000308, loss: 0.1608\n",
      "Epoch: 1, Step: 77, LR: 0.000312, loss: 0.1427\n",
      "Epoch: 1, Step: 78, LR: 0.000316, loss: 0.1480\n",
      "Epoch: 1, Step: 79, LR: 0.000320, loss: 0.1570\n",
      "Epoch: 1, Step: 80, LR: 0.000324, loss: 0.1389\n",
      "Epoch: 1, Step: 81, LR: 0.000328, loss: 0.1350\n",
      "Epoch: 1, Step: 82, LR: 0.000332, loss: 0.1226\n",
      "Epoch: 1, Step: 83, LR: 0.000336, loss: 0.1242\n",
      "Epoch: 1, Step: 84, LR: 0.000340, loss: 0.1305\n",
      "Epoch: 1, Step: 85, LR: 0.000344, loss: 0.1153\n",
      "Epoch: 1, Step: 86, LR: 0.000348, loss: 0.1175\n",
      "Epoch: 1, Step: 87, LR: 0.000352, loss: 0.1081\n",
      "Epoch: 1, Step: 88, LR: 0.000356, loss: 0.1037\n",
      "Epoch: 1, Step: 89, LR: 0.000360, loss: 0.1018\n",
      "Epoch: 1, Step: 90, LR: 0.000364, loss: 0.0986\n",
      "Epoch: 1, Step: 91, LR: 0.000368, loss: 0.0989\n",
      "Epoch: 001, loss_train: 0.7641\n",
      "Epoch: 2, Step: 0, LR: 0.000372, loss: 0.0880\n",
      "Epoch: 2, Step: 1, LR: 0.000376, loss: 0.0944\n",
      "Epoch: 2, Step: 2, LR: 0.000380, loss: 0.0822\n",
      "Epoch: 2, Step: 3, LR: 0.000384, loss: 0.0881\n",
      "Epoch: 2, Step: 4, LR: 0.000388, loss: 0.0766\n",
      "Epoch: 2, Step: 5, LR: 0.000392, loss: 0.0795\n",
      "Epoch: 2, Step: 6, LR: 0.000396, loss: 0.0786\n",
      "Epoch: 2, Step: 7, LR: 0.000400, loss: 0.0754\n",
      "Epoch: 2, Step: 8, LR: 0.000404, loss: 0.0719\n",
      "Epoch: 2, Step: 9, LR: 0.000408, loss: 0.0666\n",
      "Epoch: 2, Step: 10, LR: 0.000412, loss: 0.0603\n",
      "Epoch: 2, Step: 11, LR: 0.000416, loss: 0.0615\n",
      "Epoch: 2, Step: 12, LR: 0.000420, loss: 0.0689\n",
      "Epoch: 2, Step: 13, LR: 0.000424, loss: 0.0533\n",
      "Epoch: 2, Step: 14, LR: 0.000428, loss: 0.0489\n",
      "Epoch: 2, Step: 15, LR: 0.000432, loss: 0.0575\n",
      "Epoch: 2, Step: 16, LR: 0.000436, loss: 0.0466\n",
      "Epoch: 2, Step: 17, LR: 0.000440, loss: 0.0534\n",
      "Epoch: 2, Step: 18, LR: 0.000444, loss: 0.0487\n",
      "Epoch: 2, Step: 19, LR: 0.000448, loss: 0.0455\n",
      "Epoch: 2, Step: 20, LR: 0.000452, loss: 0.0431\n",
      "Epoch: 2, Step: 21, LR: 0.000456, loss: 0.0405\n",
      "Epoch: 2, Step: 22, LR: 0.000460, loss: 0.0381\n",
      "Epoch: 2, Step: 23, LR: 0.000464, loss: 0.0447\n",
      "Epoch: 2, Step: 24, LR: 0.000468, loss: 0.0382\n",
      "Epoch: 2, Step: 25, LR: 0.000472, loss: 0.0347\n",
      "Epoch: 2, Step: 26, LR: 0.000476, loss: 0.0329\n",
      "Epoch: 2, Step: 27, LR: 0.000480, loss: 0.0415\n",
      "Epoch: 2, Step: 28, LR: 0.000484, loss: 0.0395\n",
      "Epoch: 2, Step: 29, LR: 0.000488, loss: 0.0366\n",
      "Epoch: 2, Step: 30, LR: 0.000492, loss: 0.0336\n",
      "Epoch: 2, Step: 31, LR: 0.000496, loss: 0.0322\n",
      "Epoch: 2, Step: 32, LR: 0.000500, loss: 0.0297\n",
      "Epoch: 2, Step: 33, LR: 0.000504, loss: 0.0312\n",
      "Epoch: 2, Step: 34, LR: 0.000508, loss: 0.0286\n",
      "Epoch: 2, Step: 35, LR: 0.000512, loss: 0.0258\n",
      "Epoch: 2, Step: 36, LR: 0.000516, loss: 0.0281\n",
      "Epoch: 2, Step: 37, LR: 0.000520, loss: 0.0282\n",
      "Epoch: 2, Step: 38, LR: 0.000524, loss: 0.0242\n",
      "Epoch: 2, Step: 39, LR: 0.000528, loss: 0.0239\n",
      "Epoch: 2, Step: 40, LR: 0.000532, loss: 0.0231\n",
      "Epoch: 2, Step: 41, LR: 0.000536, loss: 0.0225\n",
      "Epoch: 2, Step: 42, LR: 0.000540, loss: 0.0225\n",
      "Epoch: 2, Step: 43, LR: 0.000544, loss: 0.0226\n",
      "Epoch: 2, Step: 44, LR: 0.000548, loss: 0.0195\n",
      "Epoch: 2, Step: 45, LR: 0.000552, loss: 0.0202\n",
      "Epoch: 2, Step: 46, LR: 0.000556, loss: 0.0163\n",
      "Epoch: 2, Step: 47, LR: 0.000560, loss: 0.0226\n",
      "Epoch: 2, Step: 48, LR: 0.000564, loss: 0.0210\n",
      "Epoch: 2, Step: 49, LR: 0.000568, loss: 0.0185\n",
      "Epoch: 2, Step: 50, LR: 0.000572, loss: 0.0172\n",
      "Epoch: 2, Step: 51, LR: 0.000576, loss: 0.0175\n",
      "Epoch: 2, Step: 52, LR: 0.000580, loss: 0.0175\n",
      "Epoch: 2, Step: 53, LR: 0.000584, loss: 0.0177\n",
      "Epoch: 2, Step: 54, LR: 0.000588, loss: 0.0154\n",
      "Epoch: 2, Step: 55, LR: 0.000592, loss: 0.0176\n",
      "Epoch: 2, Step: 56, LR: 0.000596, loss: 0.0145\n",
      "Epoch: 2, Step: 57, LR: 0.000600, loss: 0.0138\n",
      "Epoch: 2, Step: 58, LR: 0.000604, loss: 0.0127\n",
      "Epoch: 2, Step: 59, LR: 0.000608, loss: 0.0119\n",
      "Epoch: 2, Step: 60, LR: 0.000612, loss: 0.0128\n",
      "Epoch: 2, Step: 61, LR: 0.000616, loss: 0.0121\n",
      "Epoch: 2, Step: 62, LR: 0.000620, loss: 0.0114\n",
      "Epoch: 2, Step: 63, LR: 0.000624, loss: 0.0113\n",
      "Epoch: 2, Step: 64, LR: 0.000628, loss: 0.0138\n",
      "Epoch: 2, Step: 65, LR: 0.000632, loss: 0.0108\n",
      "Epoch: 2, Step: 66, LR: 0.000636, loss: 0.0105\n",
      "Epoch: 2, Step: 67, LR: 0.000640, loss: 0.0108\n",
      "Epoch: 2, Step: 68, LR: 0.000644, loss: 0.0113\n",
      "Epoch: 2, Step: 69, LR: 0.000648, loss: 0.0087\n",
      "Epoch: 2, Step: 70, LR: 0.000652, loss: 0.0103\n",
      "Epoch: 2, Step: 71, LR: 0.000656, loss: 0.0112\n",
      "Epoch: 2, Step: 72, LR: 0.000660, loss: 0.0091\n",
      "Epoch: 2, Step: 73, LR: 0.000664, loss: 0.0108\n",
      "Epoch: 2, Step: 74, LR: 0.000668, loss: 0.0094\n",
      "Epoch: 2, Step: 75, LR: 0.000672, loss: 0.0089\n",
      "Epoch: 2, Step: 76, LR: 0.000676, loss: 0.0089\n",
      "Epoch: 2, Step: 77, LR: 0.000680, loss: 0.0083\n",
      "Epoch: 2, Step: 78, LR: 0.000684, loss: 0.0085\n",
      "Epoch: 2, Step: 79, LR: 0.000688, loss: 0.0073\n",
      "Epoch: 2, Step: 80, LR: 0.000692, loss: 0.0086\n",
      "Epoch: 2, Step: 81, LR: 0.000696, loss: 0.0074\n",
      "Epoch: 2, Step: 82, LR: 0.000700, loss: 0.0065\n",
      "Epoch: 2, Step: 83, LR: 0.000704, loss: 0.0058\n",
      "Epoch: 2, Step: 84, LR: 0.000708, loss: 0.0092\n",
      "Epoch: 2, Step: 85, LR: 0.000712, loss: 0.0061\n",
      "Epoch: 2, Step: 86, LR: 0.000716, loss: 0.0063\n",
      "Epoch: 2, Step: 87, LR: 0.000720, loss: 0.0071\n",
      "Epoch: 2, Step: 88, LR: 0.000724, loss: 0.0066\n",
      "Epoch: 2, Step: 89, LR: 0.000728, loss: 0.0070\n",
      "Epoch: 2, Step: 90, LR: 0.000732, loss: 0.0064\n",
      "Epoch: 2, Step: 91, LR: 0.000736, loss: 0.0098\n",
      "Epoch: 002, loss_train: 0.0291\n",
      "Epoch: 3, Step: 0, LR: 0.000740, loss: 0.0066\n",
      "Epoch: 3, Step: 1, LR: 0.000744, loss: 0.0074\n",
      "Epoch: 3, Step: 2, LR: 0.000748, loss: 0.0073\n",
      "Epoch: 3, Step: 3, LR: 0.000752, loss: 0.0061\n",
      "Epoch: 3, Step: 4, LR: 0.000756, loss: 0.0059\n",
      "Epoch: 3, Step: 5, LR: 0.000760, loss: 0.0070\n",
      "Epoch: 3, Step: 6, LR: 0.000764, loss: 0.0063\n",
      "Epoch: 3, Step: 7, LR: 0.000768, loss: 0.0050\n",
      "Epoch: 3, Step: 8, LR: 0.000772, loss: 0.0062\n",
      "Epoch: 3, Step: 9, LR: 0.000776, loss: 0.0054\n",
      "Epoch: 3, Step: 10, LR: 0.000780, loss: 0.0054\n",
      "Epoch: 3, Step: 11, LR: 0.000784, loss: 0.0045\n",
      "Epoch: 3, Step: 12, LR: 0.000788, loss: 0.0048\n",
      "Epoch: 3, Step: 13, LR: 0.000792, loss: 0.0053\n",
      "Epoch: 3, Step: 14, LR: 0.000796, loss: 0.0052\n",
      "Epoch: 3, Step: 15, LR: 0.000800, loss: 0.0056\n",
      "Epoch: 3, Step: 16, LR: 0.000804, loss: 0.0044\n",
      "Epoch: 3, Step: 17, LR: 0.000808, loss: 0.0048\n",
      "Epoch: 3, Step: 18, LR: 0.000812, loss: 0.0046\n",
      "Epoch: 3, Step: 19, LR: 0.000816, loss: 0.0048\n",
      "Epoch: 3, Step: 20, LR: 0.000820, loss: 0.0053\n",
      "Epoch: 3, Step: 21, LR: 0.000824, loss: 0.0062\n",
      "Epoch: 3, Step: 22, LR: 0.000828, loss: 0.0047\n",
      "Epoch: 3, Step: 23, LR: 0.000832, loss: 0.0048\n",
      "Epoch: 3, Step: 24, LR: 0.000836, loss: 0.0039\n",
      "Epoch: 3, Step: 25, LR: 0.000840, loss: 0.0050\n",
      "Epoch: 3, Step: 26, LR: 0.000844, loss: 0.0049\n",
      "Epoch: 3, Step: 27, LR: 0.000848, loss: 0.0051\n",
      "Epoch: 3, Step: 28, LR: 0.000852, loss: 0.0050\n",
      "Epoch: 3, Step: 29, LR: 0.000856, loss: 0.0036\n",
      "Epoch: 3, Step: 30, LR: 0.000860, loss: 0.0048\n",
      "Epoch: 3, Step: 31, LR: 0.000864, loss: 0.0053\n",
      "Epoch: 3, Step: 32, LR: 0.000868, loss: 0.0040\n",
      "Epoch: 3, Step: 33, LR: 0.000872, loss: 0.0056\n",
      "Epoch: 3, Step: 34, LR: 0.000876, loss: 0.0041\n",
      "Epoch: 3, Step: 35, LR: 0.000880, loss: 0.0039\n",
      "Epoch: 3, Step: 36, LR: 0.000884, loss: 0.0048\n",
      "Epoch: 3, Step: 37, LR: 0.000888, loss: 0.0039\n",
      "Epoch: 3, Step: 38, LR: 0.000892, loss: 0.0034\n",
      "Epoch: 3, Step: 39, LR: 0.000896, loss: 0.0042\n",
      "Epoch: 3, Step: 40, LR: 0.000900, loss: 0.0044\n",
      "Epoch: 3, Step: 41, LR: 0.000904, loss: 0.0032\n",
      "Epoch: 3, Step: 42, LR: 0.000908, loss: 0.0047\n",
      "Epoch: 3, Step: 43, LR: 0.000912, loss: 0.0049\n",
      "Epoch: 3, Step: 44, LR: 0.000916, loss: 0.0040\n",
      "Epoch: 3, Step: 45, LR: 0.000920, loss: 0.0053\n",
      "Epoch: 3, Step: 46, LR: 0.000924, loss: 0.0040\n",
      "Epoch: 3, Step: 47, LR: 0.000928, loss: 0.0113\n",
      "Epoch: 3, Step: 48, LR: 0.000932, loss: 0.1640\n",
      "Epoch: 3, Step: 49, LR: 0.000936, loss: 16.1093\n",
      "Epoch: 3, Step: 50, LR: 0.000940, loss: 10.0700\n",
      "Epoch: 3, Step: 51, LR: 0.000944, loss: 9.0278\n",
      "Epoch: 3, Step: 52, LR: 0.000948, loss: 6.6098\n",
      "Epoch: 3, Step: 53, LR: 0.000952, loss: 1.0082\n",
      "Epoch: 3, Step: 54, LR: 0.000956, loss: 3.6218\n",
      "Epoch: 3, Step: 55, LR: 0.000960, loss: 2.7937\n",
      "Epoch: 3, Step: 56, LR: 0.000964, loss: 1.6622\n",
      "Epoch: 3, Step: 57, LR: 0.000968, loss: 0.9175\n",
      "Epoch: 3, Step: 58, LR: 0.000972, loss: 1.4158\n",
      "Epoch: 3, Step: 59, LR: 0.000976, loss: 1.3945\n",
      "Epoch: 3, Step: 60, LR: 0.000980, loss: 1.1656\n",
      "Epoch: 3, Step: 61, LR: 0.000984, loss: 0.7404\n",
      "Epoch: 3, Step: 62, LR: 0.000988, loss: 0.7473\n",
      "Epoch: 3, Step: 63, LR: 0.000992, loss: 0.9971\n",
      "Epoch: 3, Step: 64, LR: 0.000996, loss: 1.0823\n",
      "Epoch: 3, Step: 65, LR: 0.001000, loss: 0.8922\n",
      "Epoch: 3, Step: 66, LR: 0.001000, loss: 0.6326\n",
      "Epoch: 3, Step: 67, LR: 0.001000, loss: 0.5382\n",
      "Epoch: 3, Step: 68, LR: 0.001000, loss: 0.6275\n",
      "Epoch: 3, Step: 69, LR: 0.001000, loss: 0.6927\n",
      "Epoch: 3, Step: 70, LR: 0.001000, loss: 0.6486\n",
      "Epoch: 3, Step: 71, LR: 0.001000, loss: 0.5262\n",
      "Epoch: 3, Step: 72, LR: 0.001000, loss: 0.4540\n",
      "Epoch: 3, Step: 73, LR: 0.001000, loss: 0.4189\n",
      "Epoch: 3, Step: 74, LR: 0.001000, loss: 0.3968\n",
      "Epoch: 3, Step: 75, LR: 0.001000, loss: 0.4328\n",
      "Epoch: 3, Step: 76, LR: 0.001000, loss: 0.4238\n",
      "Epoch: 3, Step: 77, LR: 0.001000, loss: 0.3855\n",
      "Epoch: 3, Step: 78, LR: 0.001000, loss: 0.3155\n",
      "Epoch: 3, Step: 79, LR: 0.001000, loss: 0.3166\n",
      "Epoch: 3, Step: 80, LR: 0.001000, loss: 0.2928\n",
      "Epoch: 3, Step: 81, LR: 0.001000, loss: 0.2703\n",
      "Epoch: 3, Step: 82, LR: 0.001000, loss: 0.2757\n",
      "Epoch: 3, Step: 83, LR: 0.001000, loss: 0.2785\n",
      "Epoch: 3, Step: 84, LR: 0.001000, loss: 0.2404\n",
      "Epoch: 3, Step: 85, LR: 0.001000, loss: 0.2216\n",
      "Epoch: 3, Step: 86, LR: 0.001000, loss: 0.2119\n",
      "Epoch: 3, Step: 87, LR: 0.001000, loss: 0.2113\n",
      "Epoch: 3, Step: 88, LR: 0.001000, loss: 0.1840\n",
      "Epoch: 3, Step: 89, LR: 0.001000, loss: 0.1864\n",
      "Epoch: 3, Step: 90, LR: 0.001000, loss: 0.1927\n",
      "Epoch: 3, Step: 91, LR: 0.001000, loss: 0.1856\n",
      "Epoch: 003, loss_train: 0.7633\n",
      "Epoch: 4, Step: 0, LR: 0.001000, loss: 0.1814\n",
      "Epoch: 4, Step: 1, LR: 0.001000, loss: 0.1469\n",
      "Epoch: 4, Step: 2, LR: 0.001000, loss: 0.1480\n",
      "Epoch: 4, Step: 3, LR: 0.001000, loss: 0.1414\n",
      "Epoch: 4, Step: 4, LR: 0.001000, loss: 0.1350\n",
      "Epoch: 4, Step: 5, LR: 0.001000, loss: 0.1277\n",
      "Epoch: 4, Step: 6, LR: 0.001000, loss: 0.1336\n",
      "Epoch: 4, Step: 7, LR: 0.001000, loss: 0.1206\n",
      "Epoch: 4, Step: 8, LR: 0.001000, loss: 0.1072\n",
      "Epoch: 4, Step: 9, LR: 0.001000, loss: 0.1106\n",
      "Epoch: 4, Step: 10, LR: 0.001000, loss: 0.1027\n",
      "Epoch: 4, Step: 11, LR: 0.001000, loss: 0.1054\n",
      "Epoch: 4, Step: 12, LR: 0.001000, loss: 0.0878\n",
      "Epoch: 4, Step: 13, LR: 0.001000, loss: 0.0959\n",
      "Epoch: 4, Step: 14, LR: 0.001000, loss: 0.0971\n",
      "Epoch: 4, Step: 15, LR: 0.001000, loss: 0.0855\n",
      "Epoch: 4, Step: 16, LR: 0.001000, loss: 0.0828\n",
      "Epoch: 4, Step: 17, LR: 0.001000, loss: 0.0706\n",
      "Epoch: 4, Step: 18, LR: 0.001000, loss: 0.0719\n",
      "Epoch: 4, Step: 19, LR: 0.001000, loss: 0.0647\n",
      "Epoch: 4, Step: 20, LR: 0.001000, loss: 0.0702\n",
      "Epoch: 4, Step: 21, LR: 0.001000, loss: 0.0643\n",
      "Epoch: 4, Step: 22, LR: 0.001000, loss: 0.0831\n",
      "Epoch: 4, Step: 23, LR: 0.001000, loss: 0.0635\n",
      "Epoch: 4, Step: 24, LR: 0.001000, loss: 0.0656\n",
      "Epoch: 4, Step: 25, LR: 0.001000, loss: 0.0572\n",
      "Epoch: 4, Step: 26, LR: 0.001000, loss: 0.0575\n",
      "Epoch: 4, Step: 27, LR: 0.001000, loss: 0.0573\n",
      "Epoch: 4, Step: 28, LR: 0.001000, loss: 0.0550\n",
      "Epoch: 4, Step: 29, LR: 0.001000, loss: 0.0610\n",
      "Epoch: 4, Step: 30, LR: 0.001000, loss: 0.0502\n",
      "Epoch: 4, Step: 31, LR: 0.001000, loss: 0.0539\n",
      "Epoch: 4, Step: 32, LR: 0.001000, loss: 0.0496\n",
      "Epoch: 4, Step: 33, LR: 0.001000, loss: 0.0514\n",
      "Epoch: 4, Step: 34, LR: 0.001000, loss: 0.0430\n",
      "Epoch: 4, Step: 35, LR: 0.001000, loss: 0.0518\n",
      "Epoch: 4, Step: 36, LR: 0.001000, loss: 0.0410\n",
      "Epoch: 4, Step: 37, LR: 0.001000, loss: 0.0450\n",
      "Epoch: 4, Step: 38, LR: 0.001000, loss: 0.0443\n",
      "Epoch: 4, Step: 39, LR: 0.001000, loss: 0.0471\n",
      "Epoch: 4, Step: 40, LR: 0.001000, loss: 0.0376\n",
      "Epoch: 4, Step: 41, LR: 0.001000, loss: 0.0365\n",
      "Epoch: 4, Step: 42, LR: 0.001000, loss: 0.0309\n",
      "Epoch: 4, Step: 43, LR: 0.001000, loss: 0.0335\n",
      "Epoch: 4, Step: 44, LR: 0.001000, loss: 0.0324\n",
      "Epoch: 4, Step: 45, LR: 0.001000, loss: 0.0340\n",
      "Epoch: 4, Step: 46, LR: 0.001000, loss: 0.0350\n",
      "Epoch: 4, Step: 47, LR: 0.001000, loss: 0.0312\n",
      "Epoch: 4, Step: 48, LR: 0.001000, loss: 0.0265\n",
      "Epoch: 4, Step: 49, LR: 0.001000, loss: 0.0278\n",
      "Epoch: 4, Step: 50, LR: 0.001000, loss: 0.0275\n",
      "Epoch: 4, Step: 51, LR: 0.001000, loss: 0.0289\n",
      "Epoch: 4, Step: 52, LR: 0.001000, loss: 0.0291\n",
      "Epoch: 4, Step: 53, LR: 0.001000, loss: 0.0254\n",
      "Epoch: 4, Step: 54, LR: 0.001000, loss: 0.0271\n",
      "Epoch: 4, Step: 55, LR: 0.001000, loss: 0.0264\n",
      "Epoch: 4, Step: 56, LR: 0.001000, loss: 0.0230\n",
      "Epoch: 4, Step: 57, LR: 0.001000, loss: 0.0230\n",
      "Epoch: 4, Step: 58, LR: 0.001000, loss: 0.0250\n",
      "Epoch: 4, Step: 59, LR: 0.001000, loss: 0.0218\n",
      "Epoch: 4, Step: 60, LR: 0.001000, loss: 0.0273\n",
      "Epoch: 4, Step: 61, LR: 0.001000, loss: 0.0269\n",
      "Epoch: 4, Step: 62, LR: 0.001000, loss: 0.0255\n",
      "Epoch: 4, Step: 63, LR: 0.001000, loss: 0.0220\n",
      "Epoch: 4, Step: 64, LR: 0.001000, loss: 0.0247\n",
      "Epoch: 4, Step: 65, LR: 0.001000, loss: 0.0177\n",
      "Epoch: 4, Step: 66, LR: 0.001000, loss: 0.0217\n",
      "Epoch: 4, Step: 67, LR: 0.001000, loss: 0.0214\n",
      "Epoch: 4, Step: 68, LR: 0.001000, loss: 0.0200\n",
      "Epoch: 4, Step: 69, LR: 0.001000, loss: 0.0228\n",
      "Epoch: 4, Step: 70, LR: 0.001000, loss: 0.0182\n",
      "Epoch: 4, Step: 71, LR: 0.001000, loss: 0.0180\n",
      "Epoch: 4, Step: 72, LR: 0.001000, loss: 0.0172\n",
      "Epoch: 4, Step: 73, LR: 0.001000, loss: 0.0191\n",
      "Epoch: 4, Step: 74, LR: 0.001000, loss: 0.0183\n",
      "Epoch: 4, Step: 75, LR: 0.001000, loss: 0.0178\n",
      "Epoch: 4, Step: 76, LR: 0.001000, loss: 0.0183\n",
      "Epoch: 4, Step: 77, LR: 0.001000, loss: 0.0167\n",
      "Epoch: 4, Step: 78, LR: 0.001000, loss: 0.0134\n",
      "Epoch: 4, Step: 79, LR: 0.001000, loss: 0.0187\n",
      "Epoch: 4, Step: 80, LR: 0.001000, loss: 0.0150\n",
      "Epoch: 4, Step: 81, LR: 0.001000, loss: 0.0134\n",
      "Epoch: 4, Step: 82, LR: 0.001000, loss: 0.0121\n",
      "Epoch: 4, Step: 83, LR: 0.001000, loss: 0.0190\n",
      "Epoch: 4, Step: 84, LR: 0.001000, loss: 0.0142\n",
      "Epoch: 4, Step: 85, LR: 0.001000, loss: 0.0142\n",
      "Epoch: 4, Step: 86, LR: 0.001000, loss: 0.0144\n",
      "Epoch: 4, Step: 87, LR: 0.001000, loss: 0.0129\n",
      "Epoch: 4, Step: 88, LR: 0.001000, loss: 0.0135\n",
      "Epoch: 4, Step: 89, LR: 0.001000, loss: 0.0160\n",
      "Epoch: 4, Step: 90, LR: 0.001000, loss: 0.0127\n",
      "Epoch: 4, Step: 91, LR: 0.001000, loss: 0.0226\n",
      "Epoch: 004, loss_train: 0.0489\n",
      "Epoch: 5, Step: 0, LR: 0.001000, loss: 0.0148\n",
      "Epoch: 5, Step: 1, LR: 0.001000, loss: 0.0155\n",
      "Epoch: 5, Step: 2, LR: 0.001000, loss: 0.0160\n",
      "Epoch: 5, Step: 3, LR: 0.001000, loss: 0.0130\n",
      "Epoch: 5, Step: 4, LR: 0.001000, loss: 0.0131\n",
      "Epoch: 5, Step: 5, LR: 0.001000, loss: 0.0151\n",
      "Epoch: 5, Step: 6, LR: 0.001000, loss: 0.0119\n",
      "Epoch: 5, Step: 7, LR: 0.001000, loss: 0.0112\n",
      "Epoch: 5, Step: 8, LR: 0.001000, loss: 0.0090\n",
      "Epoch: 5, Step: 9, LR: 0.001000, loss: 0.0125\n",
      "Epoch: 5, Step: 10, LR: 0.001000, loss: 0.0121\n",
      "Epoch: 5, Step: 11, LR: 0.001000, loss: 0.0114\n",
      "Epoch: 5, Step: 12, LR: 0.001000, loss: 0.0102\n",
      "Epoch: 5, Step: 13, LR: 0.001000, loss: 0.0113\n",
      "Epoch: 5, Step: 14, LR: 0.001000, loss: 0.0113\n",
      "Epoch: 5, Step: 15, LR: 0.001000, loss: 0.0115\n",
      "Epoch: 5, Step: 16, LR: 0.001000, loss: 0.0123\n",
      "Epoch: 5, Step: 17, LR: 0.001000, loss: 0.0112\n",
      "Epoch: 5, Step: 18, LR: 0.001000, loss: 0.0110\n",
      "Epoch: 5, Step: 19, LR: 0.001000, loss: 0.0090\n",
      "Epoch: 5, Step: 20, LR: 0.001000, loss: 0.0090\n",
      "Epoch: 5, Step: 21, LR: 0.001000, loss: 0.0097\n",
      "Epoch: 5, Step: 22, LR: 0.001000, loss: 0.0115\n",
      "Epoch: 5, Step: 23, LR: 0.001000, loss: 0.0107\n",
      "Epoch: 5, Step: 24, LR: 0.001000, loss: 0.0099\n",
      "Epoch: 5, Step: 25, LR: 0.001000, loss: 0.0097\n",
      "Epoch: 5, Step: 26, LR: 0.001000, loss: 0.0085\n",
      "Epoch: 5, Step: 27, LR: 0.001000, loss: 0.0075\n",
      "Epoch: 5, Step: 28, LR: 0.001000, loss: 0.0084\n",
      "Epoch: 5, Step: 29, LR: 0.001000, loss: 0.0111\n",
      "Epoch: 5, Step: 30, LR: 0.001000, loss: 0.0101\n",
      "Epoch: 5, Step: 31, LR: 0.001000, loss: 0.0083\n",
      "Epoch: 5, Step: 32, LR: 0.001000, loss: 0.0085\n",
      "Epoch: 5, Step: 33, LR: 0.001000, loss: 0.0098\n",
      "Epoch: 5, Step: 34, LR: 0.001000, loss: 0.0084\n",
      "Epoch: 5, Step: 35, LR: 0.001000, loss: 0.0070\n",
      "Epoch: 5, Step: 36, LR: 0.001000, loss: 0.0110\n",
      "Epoch: 5, Step: 37, LR: 0.001000, loss: 0.0065\n",
      "Epoch: 5, Step: 38, LR: 0.001000, loss: 0.0070\n",
      "Epoch: 5, Step: 39, LR: 0.001000, loss: 0.0097\n",
      "Epoch: 5, Step: 40, LR: 0.001000, loss: 0.0094\n",
      "Epoch: 5, Step: 41, LR: 0.001000, loss: 0.0082\n",
      "Epoch: 5, Step: 42, LR: 0.001000, loss: 0.0073\n",
      "Epoch: 5, Step: 43, LR: 0.001000, loss: 0.0086\n",
      "Epoch: 5, Step: 44, LR: 0.001000, loss: 0.0070\n",
      "Epoch: 5, Step: 45, LR: 0.001000, loss: 0.0074\n",
      "Epoch: 5, Step: 46, LR: 0.001000, loss: 0.0071\n",
      "Epoch: 5, Step: 47, LR: 0.001000, loss: 0.0061\n",
      "Epoch: 5, Step: 48, LR: 0.001000, loss: 0.0063\n",
      "Epoch: 5, Step: 49, LR: 0.001000, loss: 0.0072\n",
      "Epoch: 5, Step: 50, LR: 0.001000, loss: 0.0060\n",
      "Epoch: 5, Step: 51, LR: 0.001000, loss: 0.0074\n",
      "Epoch: 5, Step: 52, LR: 0.001000, loss: 0.0078\n",
      "Epoch: 5, Step: 53, LR: 0.001000, loss: 0.0064\n",
      "Epoch: 5, Step: 54, LR: 0.001000, loss: 0.0074\n",
      "Epoch: 5, Step: 55, LR: 0.001000, loss: 0.0054\n",
      "Epoch: 5, Step: 56, LR: 0.001000, loss: 0.0063\n",
      "Epoch: 5, Step: 57, LR: 0.001000, loss: 0.0070\n",
      "Epoch: 5, Step: 58, LR: 0.001000, loss: 0.0079\n",
      "Epoch: 5, Step: 59, LR: 0.001000, loss: 0.0068\n",
      "Epoch: 5, Step: 60, LR: 0.001000, loss: 0.0059\n",
      "Epoch: 5, Step: 61, LR: 0.001000, loss: 0.0055\n",
      "Epoch: 5, Step: 62, LR: 0.001000, loss: 0.0051\n",
      "Epoch: 5, Step: 63, LR: 0.001000, loss: 0.0065\n",
      "Epoch: 5, Step: 64, LR: 0.001000, loss: 0.0063\n",
      "Epoch: 5, Step: 65, LR: 0.001000, loss: 0.0057\n",
      "Epoch: 5, Step: 66, LR: 0.001000, loss: 0.0060\n",
      "Epoch: 5, Step: 67, LR: 0.001000, loss: 0.0056\n",
      "Epoch: 5, Step: 68, LR: 0.001000, loss: 0.0070\n",
      "Epoch: 5, Step: 69, LR: 0.001000, loss: 0.0068\n",
      "Epoch: 5, Step: 70, LR: 0.001000, loss: 0.0054\n",
      "Epoch: 5, Step: 71, LR: 0.001000, loss: 0.0053\n",
      "Epoch: 5, Step: 72, LR: 0.001000, loss: 0.0073\n",
      "Epoch: 5, Step: 73, LR: 0.001000, loss: 0.0057\n",
      "Epoch: 5, Step: 74, LR: 0.001000, loss: 0.0067\n",
      "Epoch: 5, Step: 75, LR: 0.001000, loss: 0.0050\n",
      "Epoch: 5, Step: 76, LR: 0.001000, loss: 0.0059\n",
      "Epoch: 5, Step: 77, LR: 0.001000, loss: 0.0050\n",
      "Epoch: 5, Step: 78, LR: 0.001000, loss: 0.0047\n",
      "Epoch: 5, Step: 79, LR: 0.001000, loss: 0.0048\n",
      "Epoch: 5, Step: 80, LR: 0.001000, loss: 0.0044\n",
      "Epoch: 5, Step: 81, LR: 0.001000, loss: 0.0053\n",
      "Epoch: 5, Step: 82, LR: 0.001000, loss: 0.0048\n",
      "Epoch: 5, Step: 83, LR: 0.001000, loss: 0.0053\n",
      "Epoch: 5, Step: 84, LR: 0.001000, loss: 0.0062\n",
      "Epoch: 5, Step: 85, LR: 0.001000, loss: 0.0059\n",
      "Epoch: 5, Step: 86, LR: 0.001000, loss: 0.0040\n",
      "Epoch: 5, Step: 87, LR: 0.001000, loss: 0.0057\n",
      "Epoch: 5, Step: 88, LR: 0.001000, loss: 0.0054\n",
      "Epoch: 5, Step: 89, LR: 0.001000, loss: 0.0051\n",
      "Epoch: 5, Step: 90, LR: 0.001000, loss: 0.0046\n",
      "Epoch: 5, Step: 91, LR: 0.001000, loss: 0.0052\n",
      "Epoch: 005, loss_train: 0.0081\n",
      "Epoch: 6, Step: 0, LR: 0.001000, loss: 0.0044\n",
      "Epoch: 6, Step: 1, LR: 0.001000, loss: 0.0038\n",
      "Epoch: 6, Step: 2, LR: 0.001000, loss: 0.0060\n",
      "Epoch: 6, Step: 3, LR: 0.001000, loss: 0.0052\n",
      "Epoch: 6, Step: 4, LR: 0.001000, loss: 0.0046\n",
      "Epoch: 6, Step: 5, LR: 0.001000, loss: 0.0066\n",
      "Epoch: 6, Step: 6, LR: 0.001000, loss: 0.0052\n",
      "Epoch: 6, Step: 7, LR: 0.001000, loss: 0.0055\n",
      "Epoch: 6, Step: 8, LR: 0.001000, loss: 0.0050\n",
      "Epoch: 6, Step: 9, LR: 0.001000, loss: 0.0039\n",
      "Epoch: 6, Step: 10, LR: 0.001000, loss: 0.0042\n",
      "Epoch: 6, Step: 11, LR: 0.001000, loss: 0.0045\n",
      "Epoch: 6, Step: 12, LR: 0.001000, loss: 0.0043\n",
      "Epoch: 6, Step: 13, LR: 0.001000, loss: 0.0036\n",
      "Epoch: 6, Step: 14, LR: 0.001000, loss: 0.0061\n",
      "Epoch: 6, Step: 15, LR: 0.001000, loss: 0.0041\n",
      "Epoch: 6, Step: 16, LR: 0.001000, loss: 0.0032\n",
      "Epoch: 6, Step: 17, LR: 0.001000, loss: 0.0051\n",
      "Epoch: 6, Step: 18, LR: 0.001000, loss: 0.0044\n",
      "Epoch: 6, Step: 19, LR: 0.001000, loss: 0.0040\n",
      "Epoch: 6, Step: 20, LR: 0.001000, loss: 0.0037\n",
      "Epoch: 6, Step: 21, LR: 0.001000, loss: 0.0042\n",
      "Epoch: 6, Step: 22, LR: 0.001000, loss: 0.0034\n",
      "Epoch: 6, Step: 23, LR: 0.001000, loss: 0.0042\n",
      "Epoch: 6, Step: 24, LR: 0.001000, loss: 0.0042\n",
      "Epoch: 6, Step: 25, LR: 0.001000, loss: 0.0039\n",
      "Epoch: 6, Step: 26, LR: 0.001000, loss: 0.0037\n",
      "Epoch: 6, Step: 27, LR: 0.001000, loss: 0.0036\n",
      "Epoch: 6, Step: 28, LR: 0.001000, loss: 0.0037\n",
      "Epoch: 6, Step: 29, LR: 0.001000, loss: 0.0037\n",
      "Epoch: 6, Step: 30, LR: 0.001000, loss: 0.0034\n",
      "Epoch: 6, Step: 31, LR: 0.001000, loss: 0.0036\n",
      "Epoch: 6, Step: 32, LR: 0.001000, loss: 0.0033\n",
      "Epoch: 6, Step: 33, LR: 0.001000, loss: 0.0034\n",
      "Epoch: 6, Step: 34, LR: 0.001000, loss: 0.0031\n",
      "Epoch: 6, Step: 35, LR: 0.001000, loss: 0.0039\n",
      "Epoch: 6, Step: 36, LR: 0.001000, loss: 0.0031\n",
      "Epoch: 6, Step: 37, LR: 0.001000, loss: 0.0031\n",
      "Epoch: 6, Step: 38, LR: 0.001000, loss: 0.0046\n",
      "Epoch: 6, Step: 39, LR: 0.001000, loss: 0.0036\n",
      "Epoch: 6, Step: 40, LR: 0.001000, loss: 0.0033\n",
      "Epoch: 6, Step: 41, LR: 0.001000, loss: 0.0029\n",
      "Epoch: 6, Step: 42, LR: 0.001000, loss: 0.0046\n",
      "Epoch: 6, Step: 43, LR: 0.001000, loss: 0.0034\n",
      "Epoch: 6, Step: 44, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 6, Step: 45, LR: 0.001000, loss: 0.0034\n",
      "Epoch: 6, Step: 46, LR: 0.001000, loss: 0.0029\n",
      "Epoch: 6, Step: 47, LR: 0.001000, loss: 0.0031\n",
      "Epoch: 6, Step: 48, LR: 0.001000, loss: 0.0035\n",
      "Epoch: 6, Step: 49, LR: 0.001000, loss: 0.0041\n",
      "Epoch: 6, Step: 50, LR: 0.001000, loss: 0.0032\n",
      "Epoch: 6, Step: 51, LR: 0.001000, loss: 0.0035\n",
      "Epoch: 6, Step: 52, LR: 0.001000, loss: 0.0035\n",
      "Epoch: 6, Step: 53, LR: 0.001000, loss: 0.0031\n",
      "Epoch: 6, Step: 54, LR: 0.001000, loss: 0.0033\n",
      "Epoch: 6, Step: 55, LR: 0.001000, loss: 0.0030\n",
      "Epoch: 6, Step: 56, LR: 0.001000, loss: 0.0029\n",
      "Epoch: 6, Step: 57, LR: 0.001000, loss: 0.0034\n",
      "Epoch: 6, Step: 58, LR: 0.001000, loss: 0.0027\n",
      "Epoch: 6, Step: 59, LR: 0.001000, loss: 0.0037\n",
      "Epoch: 6, Step: 60, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 6, Step: 61, LR: 0.001000, loss: 0.0031\n",
      "Epoch: 6, Step: 62, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 6, Step: 63, LR: 0.001000, loss: 0.0029\n",
      "Epoch: 6, Step: 64, LR: 0.001000, loss: 0.0035\n",
      "Epoch: 6, Step: 65, LR: 0.001000, loss: 0.0036\n",
      "Epoch: 6, Step: 66, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 6, Step: 67, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 6, Step: 68, LR: 0.001000, loss: 0.0030\n",
      "Epoch: 6, Step: 69, LR: 0.001000, loss: 0.0024\n",
      "Epoch: 6, Step: 70, LR: 0.001000, loss: 0.0048\n",
      "Epoch: 6, Step: 71, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 6, Step: 72, LR: 0.001000, loss: 0.0037\n",
      "Epoch: 6, Step: 73, LR: 0.001000, loss: 0.0042\n",
      "Epoch: 6, Step: 74, LR: 0.001000, loss: 0.0031\n",
      "Epoch: 6, Step: 75, LR: 0.001000, loss: 0.0038\n",
      "Epoch: 6, Step: 76, LR: 0.001000, loss: 0.0026\n",
      "Epoch: 6, Step: 77, LR: 0.001000, loss: 0.0026\n",
      "Epoch: 6, Step: 78, LR: 0.001000, loss: 0.0026\n",
      "Epoch: 6, Step: 79, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 6, Step: 80, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 6, Step: 81, LR: 0.001000, loss: 0.0030\n",
      "Epoch: 6, Step: 82, LR: 0.001000, loss: 0.0026\n",
      "Epoch: 6, Step: 83, LR: 0.001000, loss: 0.0036\n",
      "Epoch: 6, Step: 84, LR: 0.001000, loss: 0.0029\n",
      "Epoch: 6, Step: 85, LR: 0.001000, loss: 0.0024\n",
      "Epoch: 6, Step: 86, LR: 0.001000, loss: 0.0034\n",
      "Epoch: 6, Step: 87, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 6, Step: 88, LR: 0.001000, loss: 0.0026\n",
      "Epoch: 6, Step: 89, LR: 0.001000, loss: 0.0030\n",
      "Epoch: 6, Step: 90, LR: 0.001000, loss: 0.0033\n",
      "Epoch: 6, Step: 91, LR: 0.001000, loss: 0.0034\n",
      "Epoch: 006, loss_train: 0.0036\n",
      "Epoch: 7, Step: 0, LR: 0.001000, loss: 0.0022\n",
      "Epoch: 7, Step: 1, LR: 0.001000, loss: 0.0034\n",
      "Epoch: 7, Step: 2, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 3, LR: 0.001000, loss: 0.0027\n",
      "Epoch: 7, Step: 4, LR: 0.001000, loss: 0.0029\n",
      "Epoch: 7, Step: 5, LR: 0.001000, loss: 0.0030\n",
      "Epoch: 7, Step: 6, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 7, Step: 7, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 7, Step: 8, LR: 0.001000, loss: 0.0024\n",
      "Epoch: 7, Step: 9, LR: 0.001000, loss: 0.0027\n",
      "Epoch: 7, Step: 10, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 7, Step: 11, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 12, LR: 0.001000, loss: 0.0019\n",
      "Epoch: 7, Step: 13, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 14, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 7, Step: 15, LR: 0.001000, loss: 0.0022\n",
      "Epoch: 7, Step: 16, LR: 0.001000, loss: 0.0024\n",
      "Epoch: 7, Step: 17, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 18, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 7, Step: 19, LR: 0.001000, loss: 0.0021\n",
      "Epoch: 7, Step: 20, LR: 0.001000, loss: 0.0019\n",
      "Epoch: 7, Step: 21, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 7, Step: 22, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 23, LR: 0.001000, loss: 0.0032\n",
      "Epoch: 7, Step: 24, LR: 0.001000, loss: 0.0029\n",
      "Epoch: 7, Step: 25, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 7, Step: 26, LR: 0.001000, loss: 0.0018\n",
      "Epoch: 7, Step: 27, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 7, Step: 28, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 7, Step: 29, LR: 0.001000, loss: 0.0029\n",
      "Epoch: 7, Step: 30, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 31, LR: 0.001000, loss: 0.0026\n",
      "Epoch: 7, Step: 32, LR: 0.001000, loss: 0.0018\n",
      "Epoch: 7, Step: 33, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 7, Step: 34, LR: 0.001000, loss: 0.0032\n",
      "Epoch: 7, Step: 35, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 36, LR: 0.001000, loss: 0.0024\n",
      "Epoch: 7, Step: 37, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 7, Step: 38, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 7, Step: 39, LR: 0.001000, loss: 0.0018\n",
      "Epoch: 7, Step: 40, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 7, Step: 41, LR: 0.001000, loss: 0.0024\n",
      "Epoch: 7, Step: 42, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 7, Step: 43, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 7, Step: 44, LR: 0.001000, loss: 0.0022\n",
      "Epoch: 7, Step: 45, LR: 0.001000, loss: 0.0024\n",
      "Epoch: 7, Step: 46, LR: 0.001000, loss: 0.0021\n",
      "Epoch: 7, Step: 47, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 48, LR: 0.001000, loss: 0.0022\n",
      "Epoch: 7, Step: 49, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 7, Step: 50, LR: 0.001000, loss: 0.0021\n",
      "Epoch: 7, Step: 51, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 7, Step: 52, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 53, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 7, Step: 54, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 55, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 7, Step: 56, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 7, Step: 57, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 58, LR: 0.001000, loss: 0.0021\n",
      "Epoch: 7, Step: 59, LR: 0.001000, loss: 0.0022\n",
      "Epoch: 7, Step: 60, LR: 0.001000, loss: 0.0018\n",
      "Epoch: 7, Step: 61, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 7, Step: 62, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 7, Step: 63, LR: 0.001000, loss: 0.0019\n",
      "Epoch: 7, Step: 64, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 7, Step: 65, LR: 0.001000, loss: 0.0015\n",
      "Epoch: 7, Step: 66, LR: 0.001000, loss: 0.0019\n",
      "Epoch: 7, Step: 67, LR: 0.001000, loss: 0.0022\n",
      "Epoch: 7, Step: 68, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 7, Step: 69, LR: 0.001000, loss: 0.0018\n",
      "Epoch: 7, Step: 70, LR: 0.001000, loss: 0.0022\n",
      "Epoch: 7, Step: 71, LR: 0.001000, loss: 0.0018\n",
      "Epoch: 7, Step: 72, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 7, Step: 73, LR: 0.001000, loss: 0.0022\n",
      "Epoch: 7, Step: 74, LR: 0.001000, loss: 0.0015\n",
      "Epoch: 7, Step: 75, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 7, Step: 76, LR: 0.001000, loss: 0.0019\n",
      "Epoch: 7, Step: 77, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 7, Step: 78, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 7, Step: 79, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 7, Step: 80, LR: 0.001000, loss: 0.0015\n",
      "Epoch: 7, Step: 81, LR: 0.001000, loss: 0.0021\n",
      "Epoch: 7, Step: 82, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 7, Step: 83, LR: 0.001000, loss: 0.0021\n",
      "Epoch: 7, Step: 84, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 7, Step: 85, LR: 0.001000, loss: 0.0036\n",
      "Epoch: 7, Step: 86, LR: 0.001000, loss: 0.0021\n",
      "Epoch: 7, Step: 87, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 7, Step: 88, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 7, Step: 89, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 7, Step: 90, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 7, Step: 91, LR: 0.001000, loss: 0.0121\n",
      "Epoch: 007, loss_train: 0.0023\n",
      "Epoch: 8, Step: 0, LR: 0.001000, loss: 0.0064\n",
      "Epoch: 8, Step: 1, LR: 0.001000, loss: 0.0194\n",
      "Epoch: 8, Step: 2, LR: 0.001000, loss: 0.0054\n",
      "Epoch: 8, Step: 3, LR: 0.001000, loss: 0.0063\n",
      "Epoch: 8, Step: 4, LR: 0.001000, loss: 0.0101\n",
      "Epoch: 8, Step: 5, LR: 0.001000, loss: 0.0084\n",
      "Epoch: 8, Step: 6, LR: 0.001000, loss: 0.0034\n",
      "Epoch: 8, Step: 7, LR: 0.001000, loss: 0.0048\n",
      "Epoch: 8, Step: 8, LR: 0.001000, loss: 0.0082\n",
      "Epoch: 8, Step: 9, LR: 0.001000, loss: 0.0043\n",
      "Epoch: 8, Step: 10, LR: 0.001000, loss: 0.0036\n",
      "Epoch: 8, Step: 11, LR: 0.001000, loss: 0.0026\n",
      "Epoch: 8, Step: 12, LR: 0.001000, loss: 0.0024\n",
      "Epoch: 8, Step: 13, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 8, Step: 14, LR: 0.001000, loss: 0.0021\n",
      "Epoch: 8, Step: 15, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 8, Step: 16, LR: 0.001000, loss: 0.0048\n",
      "Epoch: 8, Step: 17, LR: 0.001000, loss: 0.0036\n",
      "Epoch: 8, Step: 18, LR: 0.001000, loss: 0.0030\n",
      "Epoch: 8, Step: 19, LR: 0.001000, loss: 0.0039\n",
      "Epoch: 8, Step: 20, LR: 0.001000, loss: 0.0036\n",
      "Epoch: 8, Step: 21, LR: 0.001000, loss: 0.0030\n",
      "Epoch: 8, Step: 22, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 8, Step: 23, LR: 0.001000, loss: 0.0039\n",
      "Epoch: 8, Step: 24, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 8, Step: 25, LR: 0.001000, loss: 0.0022\n",
      "Epoch: 8, Step: 26, LR: 0.001000, loss: 0.0033\n",
      "Epoch: 8, Step: 27, LR: 0.001000, loss: 0.0032\n",
      "Epoch: 8, Step: 28, LR: 0.001000, loss: 0.0019\n",
      "Epoch: 8, Step: 29, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 8, Step: 30, LR: 0.001000, loss: 0.0042\n",
      "Epoch: 8, Step: 31, LR: 0.001000, loss: 0.0029\n",
      "Epoch: 8, Step: 32, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 8, Step: 33, LR: 0.001000, loss: 0.0028\n",
      "Epoch: 8, Step: 34, LR: 0.001000, loss: 0.0027\n",
      "Epoch: 8, Step: 35, LR: 0.001000, loss: 0.0021\n",
      "Epoch: 8, Step: 36, LR: 0.001000, loss: 0.0025\n",
      "Epoch: 8, Step: 37, LR: 0.001000, loss: 0.0021\n",
      "Epoch: 8, Step: 38, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 8, Step: 39, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 40, LR: 0.001000, loss: 0.0021\n",
      "Epoch: 8, Step: 41, LR: 0.001000, loss: 0.0015\n",
      "Epoch: 8, Step: 42, LR: 0.001000, loss: 0.0015\n",
      "Epoch: 8, Step: 43, LR: 0.001000, loss: 0.0019\n",
      "Epoch: 8, Step: 44, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 8, Step: 45, LR: 0.001000, loss: 0.0020\n",
      "Epoch: 8, Step: 46, LR: 0.001000, loss: 0.0018\n",
      "Epoch: 8, Step: 47, LR: 0.001000, loss: 0.0027\n",
      "Epoch: 8, Step: 48, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 8, Step: 49, LR: 0.001000, loss: 0.0015\n",
      "Epoch: 8, Step: 50, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 8, Step: 51, LR: 0.001000, loss: 0.0015\n",
      "Epoch: 8, Step: 52, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 53, LR: 0.001000, loss: 0.0023\n",
      "Epoch: 8, Step: 54, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 8, Step: 55, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 56, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 57, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 58, LR: 0.001000, loss: 0.0018\n",
      "Epoch: 8, Step: 59, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 8, Step: 60, LR: 0.001000, loss: 0.0018\n",
      "Epoch: 8, Step: 61, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 62, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 8, Step: 63, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 8, Step: 64, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 8, Step: 65, LR: 0.001000, loss: 0.0019\n",
      "Epoch: 8, Step: 66, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 8, Step: 67, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 8, Step: 68, LR: 0.001000, loss: 0.0019\n",
      "Epoch: 8, Step: 69, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 8, Step: 70, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 71, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 8, Step: 72, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 8, Step: 73, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 8, Step: 74, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 8, Step: 75, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 76, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 8, Step: 77, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 78, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 79, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 8, Step: 80, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 81, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 8, Step: 82, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 8, Step: 83, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 8, Step: 84, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 8, Step: 85, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 8, Step: 86, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 8, Step: 87, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 8, Step: 88, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 8, Step: 89, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 8, Step: 90, LR: 0.001000, loss: 0.0015\n",
      "Epoch: 8, Step: 91, LR: 0.001000, loss: 0.0034\n",
      "Epoch: 008, loss_train: 0.0026\n",
      "Epoch: 9, Step: 0, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 1, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 9, Step: 2, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 9, Step: 3, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 9, Step: 4, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 9, Step: 5, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 9, Step: 6, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 9, Step: 7, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 9, Step: 8, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 9, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 9, Step: 10, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 11, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 12, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 9, Step: 13, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 9, Step: 14, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 15, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 9, Step: 16, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 17, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 9, Step: 18, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 19, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 20, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 9, Step: 21, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 9, Step: 22, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 23, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 9, Step: 24, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 9, Step: 25, LR: 0.001000, loss: 0.0015\n",
      "Epoch: 9, Step: 26, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 9, Step: 27, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 28, LR: 0.001000, loss: 0.0016\n",
      "Epoch: 9, Step: 29, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 9, Step: 30, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 31, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 9, Step: 32, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 33, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 34, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 35, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 36, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 37, LR: 0.001000, loss: 0.0015\n",
      "Epoch: 9, Step: 38, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 9, Step: 39, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 40, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 41, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 9, Step: 42, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 43, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 44, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 9, Step: 45, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 46, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 9, Step: 47, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 48, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 49, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 9, Step: 50, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 51, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 52, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 53, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 54, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 55, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 9, Step: 56, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 57, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 9, Step: 58, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 59, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 9, Step: 60, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 61, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 62, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 9, Step: 63, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 64, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 65, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 66, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 9, Step: 67, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 68, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 69, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 9, Step: 70, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 9, Step: 71, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 72, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 73, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 9, Step: 74, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 75, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 76, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 9, Step: 77, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 9, Step: 78, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 79, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 80, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 81, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 82, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 9, Step: 83, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 9, Step: 84, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 9, Step: 85, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 86, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 87, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 9, Step: 88, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 9, Step: 89, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 9, Step: 90, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 9, Step: 91, LR: 0.001000, loss: 0.0015\n",
      "Epoch: 009, loss_train: 0.0011\n",
      "Epoch: 10, Step: 0, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 1, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 2, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 3, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 4, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 10, Step: 5, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 6, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 7, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 8, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 10, Step: 9, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 10, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 10, Step: 11, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 12, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 13, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 10, Step: 14, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 15, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 16, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 17, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 18, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 10, Step: 19, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 20, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 21, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 22, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 23, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 24, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 25, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 26, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 27, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 28, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 29, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 30, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 10, Step: 31, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 32, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 33, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 34, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 35, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 36, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 37, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 10, Step: 38, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 39, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 40, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 41, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 42, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 43, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 44, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 45, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 10, Step: 46, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 10, Step: 47, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 10, Step: 48, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 49, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 50, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 51, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 52, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 10, Step: 53, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 10, Step: 54, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 55, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 56, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 57, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 58, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 10, Step: 59, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 10, Step: 60, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 61, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 62, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 10, Step: 63, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 64, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 65, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 66, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 67, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 68, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 69, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 70, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 71, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 72, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 73, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 10, Step: 74, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 10, Step: 75, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 10, Step: 76, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 77, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 10, Step: 78, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 10, Step: 79, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 80, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 81, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 82, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 83, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 10, Step: 84, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 10, Step: 85, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 10, Step: 86, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 10, Step: 87, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 88, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 10, Step: 89, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 90, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 10, Step: 91, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 010, loss_train: 0.0008\n",
      "Epoch: 11, Step: 0, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 11, Step: 1, LR: 0.001000, loss: 0.0013\n",
      "Epoch: 11, Step: 2, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 3, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 4, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 5, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 6, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 7, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 8, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 9, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 10, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 11, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 11, Step: 12, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 13, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 14, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 15, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 16, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 17, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 18, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 19, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 11, Step: 20, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 21, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 11, Step: 22, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 11, Step: 23, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 11, Step: 24, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 11, Step: 25, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 11, Step: 26, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 27, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 11, Step: 28, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 29, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 30, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 11, Step: 31, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 32, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 33, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 34, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 35, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 36, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 11, Step: 37, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 38, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 39, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 40, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 41, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 42, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 43, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 44, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 45, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 46, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 11, Step: 47, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 48, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 49, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 50, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 51, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 11, Step: 52, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 53, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 54, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 55, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 56, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 57, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 58, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 59, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 60, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 61, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 62, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 63, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 64, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 65, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 66, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 11, Step: 67, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 68, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 69, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 11, Step: 70, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 11, Step: 71, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 11, Step: 72, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 73, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 74, LR: 0.001000, loss: 0.0011\n",
      "Epoch: 11, Step: 75, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 76, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 77, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 78, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 11, Step: 79, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 80, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 11, Step: 81, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 82, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 83, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 11, Step: 84, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 85, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 86, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 11, Step: 87, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 88, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 11, Step: 89, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 11, Step: 90, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 11, Step: 91, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 011, loss_train: 0.0007\n",
      "Epoch: 12, Step: 0, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 1, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 12, Step: 2, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 3, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 4, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 12, Step: 5, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 6, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 7, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 8, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 12, Step: 9, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 10, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 11, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 12, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 13, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 14, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 15, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 16, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 12, Step: 17, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 18, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 19, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 20, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 21, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 22, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 23, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 24, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 12, Step: 25, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 26, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 27, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 12, Step: 28, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 12, Step: 29, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 30, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 31, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 32, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 33, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 12, Step: 34, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 35, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 36, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 37, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 38, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 39, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 40, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 41, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 42, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 43, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 44, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 45, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 46, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 47, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 48, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 49, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 50, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 12, Step: 51, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 52, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 53, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 54, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 55, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 56, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 57, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 58, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 59, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 60, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 61, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 62, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 63, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 64, LR: 0.001000, loss: 0.0010\n",
      "Epoch: 12, Step: 65, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 66, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 67, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 68, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 69, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 70, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 71, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 72, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 73, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 74, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 75, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 76, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 77, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 78, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 12, Step: 79, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 80, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 12, Step: 81, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 12, Step: 82, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 83, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 84, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 85, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 86, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 12, Step: 87, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 88, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 12, Step: 89, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 12, Step: 90, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 12, Step: 91, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 012, loss_train: 0.0006\n",
      "Epoch: 13, Step: 0, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 1, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 2, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 3, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 4, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 13, Step: 5, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 6, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 7, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 8, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 9, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 10, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 11, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 13, Step: 12, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 13, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 14, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 15, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 16, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 13, Step: 17, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 13, Step: 18, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 19, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 13, Step: 20, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 21, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 22, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 23, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 13, Step: 24, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 13, Step: 25, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 26, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 27, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 28, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 29, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 30, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 13, Step: 31, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 13, Step: 32, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 13, Step: 33, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 34, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 35, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 13, Step: 36, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 37, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 13, Step: 38, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 39, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 40, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 41, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 42, LR: 0.001000, loss: 0.0009\n",
      "Epoch: 13, Step: 43, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 44, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 45, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 46, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 47, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 48, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 49, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 50, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 51, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 13, Step: 52, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 53, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 54, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 55, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 13, Step: 56, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 57, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 58, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 59, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 60, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 61, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 62, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 63, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 64, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 65, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 66, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 67, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 68, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 69, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 70, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 71, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 72, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 73, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 74, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 75, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 76, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 13, Step: 77, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 78, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 79, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 13, Step: 80, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 13, Step: 81, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 82, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 83, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 84, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 85, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 86, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 87, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 88, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 89, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 13, Step: 90, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 13, Step: 91, LR: 0.001000, loss: 0.0014\n",
      "Epoch: 013, loss_train: 0.0005\n",
      "Epoch: 14, Step: 0, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 14, Step: 1, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 2, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 3, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 4, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 5, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 6, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 7, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 8, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 14, Step: 9, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 10, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 11, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 12, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 13, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 14, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 15, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 14, Step: 16, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 17, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 14, Step: 18, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 19, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 14, Step: 20, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 21, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 22, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 23, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 24, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 25, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 26, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 27, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 28, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 29, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 30, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 31, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 32, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 33, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 34, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 35, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 36, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 37, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 38, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 39, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 40, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 41, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 14, Step: 42, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 43, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 44, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 45, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 46, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 47, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 48, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 14, Step: 49, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 50, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 51, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 52, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 53, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 54, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 55, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 56, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 14, Step: 57, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 58, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 59, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 60, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 61, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 62, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 63, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 64, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 65, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 66, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 67, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 68, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 69, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 70, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 14, Step: 71, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 72, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 73, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 74, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 75, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 76, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 77, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 14, Step: 78, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 79, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 80, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 81, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 14, Step: 82, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 83, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 84, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 85, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 14, Step: 86, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 14, Step: 87, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 14, Step: 88, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 89, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 14, Step: 90, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 14, Step: 91, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 014, loss_train: 0.0005\n",
      "Epoch: 15, Step: 0, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 1, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 15, Step: 2, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 15, Step: 3, LR: 0.001000, loss: 0.0012\n",
      "Epoch: 15, Step: 4, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 15, Step: 5, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 6, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 15, Step: 7, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 15, Step: 8, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 9, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 15, Step: 10, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 15, Step: 11, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 12, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 15, Step: 13, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 15, Step: 14, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 15, Step: 15, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 15, Step: 16, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 15, Step: 17, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 15, Step: 18, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 19, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 20, LR: 0.001000, loss: 0.0008\n",
      "Epoch: 15, Step: 21, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 15, Step: 22, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 15, Step: 23, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 24, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 25, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 15, Step: 26, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 27, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 28, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 29, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 30, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 31, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 32, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 33, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 15, Step: 34, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 15, Step: 35, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 36, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 15, Step: 37, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 38, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 39, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 40, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 15, Step: 41, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 42, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 15, Step: 43, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 15, Step: 44, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 45, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 15, Step: 46, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 47, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 48, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 49, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 50, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 51, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 15, Step: 52, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 15, Step: 53, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 15, Step: 54, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 55, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 15, Step: 56, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 57, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 58, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 59, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 60, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 61, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 62, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 63, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 64, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 65, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 66, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 15, Step: 67, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 68, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 15, Step: 69, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 70, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 71, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 72, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 73, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 74, LR: 0.001000, loss: 0.0003\n",
      "Epoch: 15, Step: 75, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 15, Step: 76, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 77, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 78, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 79, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 80, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 81, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 82, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 83, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 84, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 85, LR: 0.001000, loss: 0.0004\n",
      "Epoch: 15, Step: 86, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 87, LR: 0.001000, loss: 0.0006\n",
      "Epoch: 15, Step: 88, LR: 0.001000, loss: 0.0002\n",
      "Epoch: 15, Step: 89, LR: 0.001000, loss: 0.0005\n",
      "Epoch: 15, Step: 90, LR: 0.001000, loss: 0.0007\n",
      "Epoch: 15, Step: 91, LR: 0.001000, loss: 0.0017\n",
      "Epoch: 015, loss_train: 0.0005\n"
     ]
    }
   ],
   "source": [
    "#dgi loop\n",
    "#loop\n",
    "log_training_parameters(batch_size, learning_rate, weight_decay, epochs, neighbors_per_node, num_layers, output_dim, activation, normalization)\n",
    "# patience = 8\n",
    "# epochs_without_improvement = 0\n",
    "# best_loss = 100000\n",
    "for epoch in range (1, epochs +1):\n",
    "  loss = train_DGI()\n",
    "  # loss_valid = model\n",
    "  # if (loss_valid < best_loss):\n",
    "  #   best_loss = loss_valid\n",
    "  #   epochs_without_improvement = 0\n",
    "  #   best_model = model.state_dict()\n",
    "  # else:\n",
    "  #   epochs_without_improvement += 1\n",
    "  #   if (epochs_without_improvement == patience):\n",
    "  #     print (f'Early stopping at epoch {epoch}')\n",
    "  #     break\n",
    "  log_epoch_performance(epoch, loss, 0, 0, 0, epochs, output_dim, learning_rate, weight_decay, neighbors_per_node, num_layers, normalization)\n",
    "  writer.add_scalar('loss train',loss, epoch)\n",
    "  print (f'Epoch: {epoch:03d}, loss_train: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "AT_APkoXRm82"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model)\n",
    "folder_path = f'/content/Graph-Machine-Learning/Models/{model_name}/'\n",
    "if use_embedding:\n",
    "  folder_path = os.path.join(folder_path, 'with_embedding/')\n",
    "if pre_norm:\n",
    "  folder_path = os.path.join(folder_path, 'with_pre_norm/')\n",
    "if residual_connections:\n",
    "  folder_path = os.path.join(folder_path, 'with_residual_connections/')\n",
    "file_path = os.path.join(folder_path, f'{layer_type}_{batch_size}_batchSize_{normalization}_{activation}_{neighbors_per_node}nxn_embed_{embedding_dim}_out.pth')\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njY_qwN1DNlN",
    "outputId": "f07b3c76-8133-4e53-b832-cef764726b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test AUC: 0.7863, test AP: 0.7311\n"
     ]
    }
   ],
   "source": [
    "auc, ap = test(test_pos_edge_index, test_neg_edge_index)\n",
    "print(f'test AUC: {auc:.4f}, test AP: {ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJxTd7Upe5sZ"
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"/content/Graph-Machine-Learning/Models/new/model_GAE_1024_batchSize_batch_20nxn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEU_0PiXfsso"
   },
   "outputs": [],
   "source": [
    "subset_indices = torch.randperm(data.num_nodes)[:5000]\n",
    "subset_x = data.x[subset_indices]  # Feature matrix dei nodi selezionati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lT9FI_wih8yj"
   },
   "outputs": [],
   "source": [
    "subset_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0lUYUy-8yx1"
   },
   "outputs": [],
   "source": [
    "ground_truth = torch.argmax(data.x, dim=1)\n",
    "ground_truth_list = ground_truth.tolist()\n",
    "data.y = ground_truth_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDJj3pQpkPkz"
   },
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: \"disease\",\n",
    "    1: \"drug\",\n",
    "    2: \"function\",\n",
    "    3: \"protein\",\n",
    "    4: \"sideeffect\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kixy5Bwi9DLM"
   },
   "outputs": [],
   "source": [
    "def plot_tsne(features, labels, class_names):\n",
    "    pca = PCA(n_components=30)\n",
    "    data_pca = pca.fit_transform(features)\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_result = tsne.fit_transform(data_pca)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    # Ottieni i colori per ogni classe\n",
    "    for class_label in np.unique(labels):\n",
    "        indices = labels == class_label\n",
    "        class_name = class_names.get(class_label, f\"Class {class_label}\")\n",
    "        plt.scatter(tsne_result[indices, 0], tsne_result[indices, 1], label=class_name, alpha=0.7)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('t-SNE plot of node features')\n",
    "    cartella = f'runs/GAE_ogbl-biokg_experiment_{output_dim}d_{epochs}_epochs_{learning_rate}_lr_{weight_decay}_weight_decay_{neighbors_per_node}_num_neighbors_{num_layers}_numLayer_{normalization}_rv'\n",
    "    file_path = os.path.join(cartella, \"TSNE.png\")\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANP96eVFCmcQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33o_xsv69GWm"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  z = model.encode(data.x.to(device), data.edge_index.to(device))\n",
    "  subset_embeddings = z[subset_indices].cpu().detach().numpy()\n",
    "  labels = np.argmax(subset_x, axis=1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqzwlMlQ9W0p"
   },
   "outputs": [],
   "source": [
    "plot_tsne(subset_embeddings, labels, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0YozN2WEXo2"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
