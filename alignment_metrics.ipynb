{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXuFn1JuNPBn",
        "outputId": "7f5414ab-c897-4a91-e9d9-b64b5e81d754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Graph-Machine-Learning'...\n",
            "remote: Enumerating objects: 718, done.\u001b[K\n",
            "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 718 (delta 46), reused 73 (delta 15), pack-reused 592 (from 1)\u001b[K\n",
            "Receiving objects: 100% (718/718), 85.87 MiB | 11.27 MiB/s, done.\n",
            "Resolving deltas: 100% (347/347), done.\n",
            "Updating files: 100% (139/139), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://token@github.com/filippostaffoni/Graph-Machine-Learning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKALJVBfNaFs",
        "outputId": "9d014c9c-b45f-4f6d-f629-d3e7f913f03c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Graph-Machine-Learning\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Graph-Machine-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mA458gkhqCo2"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"filippostaffoni@hotmail.it\"\n",
        "!git config --global user.name \"Filippo Staffoni\"\n",
        "!git config pull.rebase false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sekvo44wqNHz",
        "outputId": "9d8900f7-f331-4f2b-e774-9e1cff46949e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[main 140d06f] yaml updates\n",
            " 1 file changed, 10 insertions(+), 8 deletions(-)\n",
            "Enumerating objects: 5, done.\n",
            "Counting objects: 100% (5/5), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (3/3), 412 bytes | 412.00 KiB/s, done.\n",
            "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/filippostaffoni/Graph-Machine-Learning.git\n",
            "   72aab5f..140d06f  main -> main\n"
          ]
        }
      ],
      "source": [
        "!git add --all\n",
        "!git commit -m \"yaml updates\"\n",
        "!git push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6SrdcnETcIC",
        "outputId": "df2d8dc0-d7b1-421c-a13a-91dc4a6968ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.11)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2024.12.14)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/pyg_lib-0.4.0%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_scatter-2.1.2%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_sparse-0.6.18%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_cluster-1.6.3%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (989 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m989.8/989.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (1.26.4)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt24cu121 torch_cluster-1.6.3+pt24cu121 torch_scatter-2.1.2+pt24cu121 torch_sparse-0.6.18+pt24cu121 torch_spline_conv-1.2.2+pt24cu121\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "!pip install umap-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk0UWrOD5hUd",
        "outputId": "6e2a09e2-9f78-4af0-b91e-5c590c488494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymp\n",
            "  Downloading pymp-0.0.6.tar.gz (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.11/dist-packages (from pymp) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from pymp) (1.13.1)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from pymp) (2.2.2)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.11/dist-packages (from pymp) (0.60.0)\n",
            "Requirement already satisfied: param>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from pymp) (2.2.0)\n",
            "Requirement already satisfied: panel>=0.13.1 in /usr/local/lib/python3.11/dist-packages (from pymp) (1.6.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from pymp) (8.3.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.48.0->pymp) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.0->pymp) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.0->pymp) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.0->pymp) (2025.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from panel>=0.13.1->pymp) (6.2.0)\n",
            "Requirement already satisfied: bokeh<3.7.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from panel>=0.13.1->pymp) (3.6.2)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=0.13.1->pymp) (2.0.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from panel>=0.13.1->pymp) (3.7)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=0.13.1->pymp) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from panel>=0.13.1->pymp) (0.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from panel>=0.13.1->pymp) (24.2)\n",
            "Requirement already satisfied: pyviz-comms>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from panel>=0.13.1->pymp) (3.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from panel>=0.13.1->pymp) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from panel>=0.13.1->pymp) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from panel>=0.13.1->pymp) (4.12.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->pymp) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->pymp) (1.5.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.7.0,>=3.5.0->panel>=0.13.1->pymp) (3.1.5)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.7.0,>=3.5.0->panel>=0.13.1->pymp) (1.3.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.7.0,>=3.5.0->panel>=0.13.1->pymp) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.7.0,>=3.5.0->panel>=0.13.1->pymp) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.7.0,>=3.5.0->panel>=0.13.1->pymp) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.7.0,>=3.5.0->panel>=0.13.1->pymp) (2025.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.0->pymp) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->panel>=0.13.1->pymp) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py->panel>=0.13.1->pymp) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py->panel>=0.13.1->pymp) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=0.13.1->pymp) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=0.13.1->pymp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=0.13.1->pymp) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=0.13.1->pymp) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=2.9->bokeh<3.7.0,>=3.5.0->panel>=0.13.1->pymp) (3.0.2)\n",
            "Building wheels for collected packages: pymp\n",
            "  Building wheel for pymp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymp: filename=pymp-0.0.6-py3-none-any.whl size=10732629 sha256=fb8addd2ffea22e122c23f986879368872dde02a32d1d07b27a4a67d93560403\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/e4/9f/1acf3ab0cc1bdfd508e40de00852a41f12685b5c18d0dc1d8e\n",
            "Successfully built pymp\n",
            "Installing collected packages: pymp\n",
            "Successfully installed pymp-0.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pymp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eofkyGQrTw8z"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "import os\n",
        "\n",
        "import torch.nn.functional as F  # Import functional API of PyTorch for activation functions, loss functions, etc.\n",
        "import torch.nn\n",
        "from torch.nn import BatchNorm1d, LayerNorm\n",
        "\n",
        "import torch_geometric\n",
        "import torch_geometric.transforms as T  # Import geometric-specific transforms from PyTorch Geometric\n",
        "from torch_geometric.datasets import Planetoid  # Import Planetoid dataset class\n",
        "from torch_geometric.nn import SplineConv, GCNConv, GATConv, SAGEConv, DeepGraphInfomax, VGAE  # Import SplineConv layer from PyTorch Geometric\n",
        "from torch_geometric.typing import WITH_TORCH_SPLINE_CONV  # Check if SplineConv is available\n",
        "from torch_geometric.utils import train_test_split_edges\n",
        "import xgboost as xg\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torch_geometric.data import Data, HeteroData\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "import yaml\n",
        "from torch_geometric.utils import negative_sampling\n",
        "from torch_geometric.utils import dropout_edge\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.nn import GAE, VGAE\n",
        "import torch.nn as nn\n",
        "#from load_ogbl_biokg_homo import load_ogbl_biokg_homo as load_ogbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B9F_jEaY3gQ3"
      },
      "outputs": [],
      "source": [
        "class AlignmentMetrics:\n",
        "\n",
        "    SUPPORTED_METRICS = [\n",
        "        #\"cycle_knn\",\n",
        "        \"mutual_knn\",\n",
        "        #\"lcs_knn\",\n",
        "        \"cka\",\n",
        "        #\"unbiased_cka\",\n",
        "        \"cknna\",\n",
        "        #\"svcca\",\n",
        "        #\"edit_distance_knn\",\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QP9uZJgG5Spw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio.functional as TAF\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cross_decomposition import CCA\n",
        "\n",
        "try:\n",
        "    import pymp\n",
        "    pymp_available = True\n",
        "except ImportError:\n",
        "    pymp_available = False\n",
        "    print(\"Please install the pymp library using `pip install pymp` to speed up non-batched metrics\")\n",
        "\n",
        "\n",
        "class AlignmentMetrics:\n",
        "\n",
        "    SUPPORTED_METRICS = [\n",
        "        #\"cycle_knn\",\n",
        "        \"mutual_knn\",\n",
        "        #\"lcs_knn\",\n",
        "        \"cka\",\n",
        "        #\"unbiased_cka\",\n",
        "        \"cknna\",\n",
        "        #\"svcca\",\n",
        "        #\"edit_distance_knn\",\n",
        "    ]\n",
        "\n",
        "    @staticmethod\n",
        "    def measure(metric, *args, **kwargs):\n",
        "        \"\"\" metric is a string for the function \"\"\"\n",
        "\n",
        "        if metric not in AlignmentMetrics.SUPPORTED_METRICS:\n",
        "            raise ValueError(f\"Unrecognized metric: {metric}\")\n",
        "\n",
        "        return getattr(AlignmentMetrics, metric)(*args, **kwargs)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def cycle_knn(feats_A, feats_B, topk):\n",
        "        \"\"\"\n",
        "        LLM nearest neighbors -> Query Language Pair -> LVM nearest neighbors\n",
        "        Args:\n",
        "            feats_A: A torch tensor of shape N x feat_dim\n",
        "            feats_B: A torch tensor of shape N x feat_dim\n",
        "\n",
        "        Returns:\n",
        "            acc: a float representing the accuracy\n",
        "        \"\"\"\n",
        "        knn_A = compute_nearest_neighbors(feats_A, topk)\n",
        "        knn_B = compute_nearest_neighbors(feats_B, topk)\n",
        "        return compute_knn_accuracy(knn_A[knn_B]).item()\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def mutual_knn(feats_A, feats_B, topk):\n",
        "        \"\"\"\n",
        "        Computes the mutual KNN accuracy.\n",
        "\n",
        "        Args:\n",
        "            feats_A: A torch tensor of shape N x feat_dim\n",
        "            feats_B: A torch tensor of shape N x feat_dim\n",
        "\n",
        "        Returns:\n",
        "            A float representing the mutual KNN accuracy\n",
        "        \"\"\"\n",
        "        knn_A = compute_nearest_neighbors(feats_A, topk)\n",
        "        knn_B = compute_nearest_neighbors(feats_B, topk)\n",
        "\n",
        "        n = knn_A.shape[0]\n",
        "        topk = knn_A.shape[1]\n",
        "\n",
        "        # Create a range tensor for indexing\n",
        "        range_tensor = torch.arange(n, device=knn_A.device).unsqueeze(1)\n",
        "\n",
        "        # Create binary masks for knn_A and knn_B\n",
        "        lvm_mask = torch.zeros(n, n, device=knn_A.device)\n",
        "        llm_mask = torch.zeros(n, n, device=knn_A.device)\n",
        "\n",
        "        lvm_mask[range_tensor, knn_A] = 1.0\n",
        "        llm_mask[range_tensor, knn_B] = 1.0\n",
        "\n",
        "        acc = (lvm_mask * llm_mask).sum(dim=1) / topk\n",
        "\n",
        "        return acc.mean().item()\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def lcs_knn(feats_A, feats_B, topk):\n",
        "        knn_A = compute_nearest_neighbors(feats_A, topk)\n",
        "        knn_B = compute_nearest_neighbors(feats_B, topk)\n",
        "        score = longest_ordinal_sequence(knn_A, knn_B).float().mean()\n",
        "        return score\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def cka(feats_A, feats_B, kernel_metric='ip', rbf_sigma=1.0, unbiased=False):\n",
        "        \"\"\"Computes the unbiased Centered Kernel Alignment (CKA) between features.\"\"\"\n",
        "\n",
        "        if kernel_metric == 'ip':\n",
        "            # Compute kernel matrices for the linear case\n",
        "            K = torch.mm(feats_A, feats_A.T)\n",
        "            L = torch.mm(feats_B, feats_B.T)\n",
        "        elif kernel_metric == 'rbf':\n",
        "            # COMPUTES RBF KERNEL\n",
        "            K = torch.exp(-torch.cdist(feats_A, feats_A) ** 2 / (2 * rbf_sigma ** 2))\n",
        "            L = torch.exp(-torch.cdist(feats_B, feats_B) ** 2 / (2 * rbf_sigma ** 2))\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid kernel metric {kernel_metric}\")\n",
        "\n",
        "        # Compute HSIC values\n",
        "        hsic_fn = hsic_unbiased if unbiased else hsic_biased\n",
        "        hsic_kk = hsic_fn(K, K)\n",
        "        hsic_ll = hsic_fn(L, L)\n",
        "        hsic_kl = hsic_fn(K, L)\n",
        "\n",
        "        # Compute CKA\n",
        "        #print('hsic', hsic_kl)\n",
        "        cka_value = hsic_kl / (torch.sqrt(hsic_kk * hsic_ll) + 1e-6)\n",
        "        return cka_value.item()\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def unbiased_cka(*args, **kwargs):\n",
        "        kwargs['unbiased'] = True\n",
        "        return AlignmentMetrics.cka(*args, **kwargs)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def svcca(feats_A, feats_B, cca_dim=10):\n",
        "\n",
        "        # Center and scale the activations\n",
        "        def preprocess_activations(act):\n",
        "            act = act - torch.mean(act, axis=0)\n",
        "            act = act / (torch.std(act, axis=0) + 1e-8)\n",
        "            return act\n",
        "\n",
        "        feats_A = preprocess_activations(feats_A)\n",
        "        feats_B = preprocess_activations(feats_B)\n",
        "\n",
        "        # Compute SVD\n",
        "        U1, _, _ = torch.svd_lowrank(feats_A, q=cca_dim)\n",
        "        U2, _, _ = torch.svd_lowrank(feats_B, q=cca_dim)\n",
        "\n",
        "        U1 = U1.cpu().detach().numpy()\n",
        "        U2 = U2.cpu().detach().numpy()\n",
        "\n",
        "        # Compute CCA\n",
        "        cca = CCA(n_components=cca_dim)\n",
        "        cca.fit(U1, U2)\n",
        "        U1_c, U2_c = cca.transform(U1, U2)\n",
        "\n",
        "        # sometimes it goes to nan, this is just to avoid that\n",
        "        U1_c += 1e-10 * np.random.randn(*U1_c.shape)\n",
        "        U2_c += 1e-10 * np.random.randn(*U2_c.shape)\n",
        "\n",
        "        # Compute SVCCA similarity\n",
        "        svcca_similarity = np.mean(\n",
        "            [np.corrcoef(U1_c[:, i], U2_c[:, i])[0, 1] for i in range(cca_dim)]\n",
        "        )\n",
        "        return svcca_similarity\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def edit_distance_knn(feats_A, feats_B, topk):\n",
        "        \"\"\"\n",
        "        Computes the edit distance between the nearest neighbors of feats_A and feats_B.\n",
        "        \"\"\"\n",
        "        knn_A = compute_nearest_neighbors(feats_A, topk)\n",
        "        knn_B = compute_nearest_neighbors(feats_B, topk)\n",
        "\n",
        "        # given N x topk with integer entries, compute edit distance\n",
        "        n = knn_A.shape[0]\n",
        "        topk = knn_A.shape[1]\n",
        "\n",
        "        edit_distance = compute_distance(knn_A, knn_B, TAF.edit_distance)\n",
        "        return 1 - torch.mean(edit_distance) / topk\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def cknna(feats_A, feats_B, topk=None, distance_agnostic=False, unbiased=True):\n",
        "        \"\"\" similarity only cka variant \"\"\"\n",
        "        n = feats_A.shape[0]\n",
        "\n",
        "        if topk < 2:\n",
        "            raise ValueError(\"CKNNA requires topk >= 2\")\n",
        "\n",
        "        if topk is None:\n",
        "            topk = feats_A.shape[0] - 1\n",
        "\n",
        "        K = feats_A @ feats_A.T\n",
        "        L = feats_B @ feats_B.T\n",
        "        device = feats_A.device\n",
        "\n",
        "        def similarity(K, L, topk):\n",
        "            if unbiased:\n",
        "                K_hat = K.clone().fill_diagonal_(float(\"-inf\"))\n",
        "                L_hat = L.clone().fill_diagonal_(float(\"-inf\"))\n",
        "            else:\n",
        "                K_hat, L_hat = K, L\n",
        "\n",
        "            # get topk indices for each row\n",
        "            # if unbiased we cannot attend to the diagonal unless full topk\n",
        "            # else we can attend to the diagonal\n",
        "            _, topk_K_indices = torch.topk(K_hat, topk, dim=1)\n",
        "            _, topk_L_indices = torch.topk(L_hat, topk, dim=1)\n",
        "\n",
        "            # create masks for nearest neighbors\n",
        "            mask_K = torch.zeros(n, n, device=device).scatter_(1, topk_K_indices, 1)\n",
        "            mask_L = torch.zeros(n, n, device=device).scatter_(1, topk_L_indices, 1)\n",
        "\n",
        "            # intersection of nearest neighbors\n",
        "            mask = mask_K * mask_L\n",
        "\n",
        "            if distance_agnostic:\n",
        "                sim = mask * 1.0\n",
        "            else:\n",
        "                if unbiased:\n",
        "                    sim = hsic_unbiased(mask * K, mask * L)\n",
        "                else:\n",
        "                    sim = hsic_biased(mask * K, mask * L)\n",
        "            return sim\n",
        "\n",
        "        sim_kl = similarity(K, L, topk)\n",
        "        sim_kk = similarity(K, K, topk)\n",
        "        sim_ll = similarity(L, L, topk)\n",
        "\n",
        "        return sim_kl.item() / (torch.sqrt(sim_kk * sim_ll) + 1e-6).item()\n",
        "\n",
        "\n",
        "def hsic_unbiased(K, L):\n",
        "    \"\"\"\n",
        "    Compute the unbiased Hilbert-Schmidt Independence Criterion (HSIC) as per Equation 5 in the paper.\n",
        "    > Reference: https://jmlr.csail.mit.edu/papers/volume13/song12a/song12a.pdf\n",
        "    \"\"\"\n",
        "    m = K.shape[0]\n",
        "\n",
        "    # Zero out the diagonal elements of K and L\n",
        "    K_tilde = K.clone().fill_diagonal_(0)\n",
        "    L_tilde = L.clone().fill_diagonal_(0)\n",
        "\n",
        "    # Compute HSIC using the formula in Equation 5\n",
        "    HSIC_value = (\n",
        "        (torch.sum(K_tilde * L_tilde.T))\n",
        "        + (torch.sum(K_tilde) * torch.sum(L_tilde) / ((m - 1) * (m - 2)))\n",
        "        - (2 * torch.sum(torch.mm(K_tilde, L_tilde)) / (m - 2))\n",
        "    )\n",
        "\n",
        "    HSIC_value /= m * (m - 3)\n",
        "    return HSIC_value\n",
        "\n",
        "\n",
        "def hsic_biased(K, L):\n",
        "    \"\"\" Compute the biased HSIC (the original CKA) \"\"\"\n",
        "    H = torch.eye(K.shape[0], dtype=K.dtype, device=K.device) - 1 / K.shape[0]\n",
        "    return torch.trace(K @ H @ L @ H)\n",
        "\n",
        "\n",
        "def compute_knn_accuracy(knn):\n",
        "    \"\"\"\n",
        "    Compute the accuracy of the nearest neighbors. Assumes index is the gt label.\n",
        "    Args:\n",
        "        knn: a torch tensor of shape N x topk\n",
        "    Returns:\n",
        "        acc: a float representing the accuracy\n",
        "    \"\"\"\n",
        "    n = knn.shape[0]\n",
        "    acc = knn == torch.arange(n, device=knn.device).view(-1, 1, 1)\n",
        "    acc = acc.float().view(n, -1).max(dim=1).values.mean()\n",
        "    return acc\n",
        "\n",
        "\n",
        "def compute_nearest_neighbors(feats, topk=1):\n",
        "    \"\"\"\n",
        "    Compute the nearest neighbors of feats\n",
        "    Args:\n",
        "        feats: a torch tensor of shape N x D\n",
        "        topk: the number of nearest neighbors to return\n",
        "    Returns:\n",
        "        knn: a torch tensor of shape N x topk\n",
        "    \"\"\"\n",
        "    assert feats.ndim == 2, f\"Expected feats to be 2D, got {feats.ndim}\"\n",
        "    knn = (\n",
        "        (feats @ feats.T).fill_diagonal_(-1e8).argsort(dim=1, descending=True)[:, :topk]\n",
        "    )\n",
        "    return knn\n",
        "\n",
        "\n",
        "def longest_ordinal_sequence(X, Y):\n",
        "    \"\"\" For each pair in X and Y, compute the length of the longest sub-sequence (LCS) \"\"\"\n",
        "\n",
        "    def lcs_length(x, y):\n",
        "        \"\"\"\n",
        "        Compute the length of the longest common subsequence between two sequences.\n",
        "        This is a classic dynamic programming implementation.\n",
        "        \"\"\"\n",
        "        m, n = len(x), len(y)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if x[i - 1] == y[j - 1]:\n",
        "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "                else:\n",
        "                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
        "        return dp[m][n]\n",
        "\n",
        "    lcs = compute_distance(X, Y, lcs_length)\n",
        "    return lcs\n",
        "\n",
        "\n",
        "def compute_distance(X, Y, dist_fn):\n",
        "    \"\"\" compute distance in parallel\"\"\"\n",
        "    B, N = X.shape\n",
        "    distances = np.zeros(B)\n",
        "    X, Y = X.cpu().numpy(), Y.cpu().numpy()\n",
        "\n",
        "    if pymp_available:\n",
        "        with pymp.Parallel(4) as p:\n",
        "            for i in p.range(B):\n",
        "                distances[i] = dist_fn(X[i], Y[i])\n",
        "    else:\n",
        "        for i in range(B):\n",
        "            distances[i] = dist_fn(X[i], Y[i])\n",
        "    return torch.tensor(distances)\n",
        "\n",
        "\n",
        "def remove_outliers(feats, q, exact=False, max_threshold=None):\n",
        "    if q == 1:\n",
        "        return feats\n",
        "\n",
        "    if exact:\n",
        "        # sorts the whole tensor and gets the q-th percentile\n",
        "        q_val = feats.view(-1).abs().sort().values[int(q * feats.numel())]\n",
        "    else:\n",
        "        # quantile for element in the tensor and take the average\n",
        "        q_val = torch.quantile(feats.abs().flatten(start_dim=1), q, dim=1).mean()\n",
        "\n",
        "    if max_threshold is not None:\n",
        "        max_threshold = max(max_threshold, q_val)\n",
        "\n",
        "    return feats.clamp(-q_val, q_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ivMugdtx56jn"
      },
      "outputs": [],
      "source": [
        "from torch_geometric import seed_everything\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IRvTVtW5dxF",
        "outputId": "929e0930-1c0b-4adf-ee82-d0bddb702d5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-36-f3e9396cc8e5>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load('load_data/data.pt')\n"
          ]
        }
      ],
      "source": [
        "data = torch.load('load_data/data.pt')\n",
        "\n",
        "with open('align.yaml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "activation_functions = {\n",
        "    \"relu\": torch.nn.ReLU(),\n",
        "    \"elu\": torch.nn.ELU(),\n",
        "    \"gelu\": torch.nn.GELU(),\n",
        "    \"swiglu\": torch.nn.SiLU()  # SiLU è equivalente a SwigLU in PyTorch\n",
        "}\n",
        "\n",
        "layer_types = {\n",
        "    \"GCN\": GCNConv,\n",
        "    \"SAGE\": SAGEConv,\n",
        "    \"GAT\": GATConv\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vPtxoDnlTwtg"
      },
      "outputs": [],
      "source": [
        "#per file yaml\n",
        "class GAEEncoder(torch.nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, output_dim, num_layers, activation, normalization, use_embedding, pre_norm, residual_connections, scale_factor, layer_type):\n",
        "        super(GAEEncoder, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        self.normalizations = torch.nn.ModuleList()\n",
        "        self.activation = activation_functions[activation]\n",
        "        self.use_embedding = use_embedding\n",
        "        self.pre_norm = pre_norm\n",
        "        self.residual_connections = residual_connections\n",
        "        self.scale_factor = scale_factor\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.normalization = normalization\n",
        "        self.ConvLayer = layer_types[layer_type]\n",
        "\n",
        "\n",
        "        last = False\n",
        "        if use_embedding:\n",
        "          self.class_embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
        "          input_dim = embedding_dim\n",
        "        self.layers.append(self.ConvLayer(input_dim, self.compute_new_dim(input_dim, last)))\n",
        "        curr_dim = self.compute_new_dim(input_dim, last)\n",
        "\n",
        "        if self.pre_norm_cond and self.use_embedding:\n",
        "          if self.normalization == \"batch\":\n",
        "            self.pre_norm = BatchNorm1d(embedding_dim)\n",
        "          elif self.normalization == \"layer\":\n",
        "            self.pre_norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        for i in range(num_layers - 1):\n",
        "          if normalization == \"batch\":\n",
        "            self.normalizations.append(BatchNorm1d(curr_dim))\n",
        "          elif normalization == \"layer\":\n",
        "            self.normalizations.append(LayerNorm(curr_dim))\n",
        "          elif normalization == \"none\":\n",
        "            self.normalizations.append(None)\n",
        "          else:\n",
        "            raise ValueError(f\"Unsupported normalization type: {normalization}\")\n",
        "          if self.residual_connections:\n",
        "            if self.normalization == \"batch\":\n",
        "              self.normalizations.append(BatchNorm1d(curr_dim))\n",
        "            elif self.normalization == \"layer\":\n",
        "              self.normalizations.append(LayerNorm(curr_dim))\n",
        "          if (i == num_layers - 2):\n",
        "            last = True\n",
        "          self.layers.append(self.ConvLayer(curr_dim, self.compute_new_dim(curr_dim, last)))\n",
        "          curr_dim = self.compute_new_dim(curr_dim, last)\n",
        "\n",
        "  def compute_new_dim(self, current_dim, last):\n",
        "    if last:\n",
        "        return self.output_dim\n",
        "    if current_dim < self.output_dim:\n",
        "        return self.embedding_dim\n",
        "    if self.scale_factor == \"half\":\n",
        "        return int((current_dim + self.output_dim)/2)\n",
        "    else:\n",
        "        return current_dim\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    if self.use_embedding:\n",
        "      node_classes = torch.argmax(x, dim=1)\n",
        "      x = self.class_embedding(node_classes)\n",
        "    if self.pre_norm_cond and self.use_embedding:\n",
        "      x = self.pre_norm(x)\n",
        "    if self.residual_connections:\n",
        "      for i, conv in enumerate(self.layers):\n",
        "          x = x + self.activation(conv(self.normalizations[i](x), edge_index))\n",
        "    else:\n",
        "      for i, conv in enumerate(self.layers):\n",
        "          x = conv(x, edge_index)\n",
        "      if i < len(self.layers) - 1:\n",
        "        if self.normalizations[i] is not None:  # Normalizzazione, se definita\n",
        "            x = self.normalizations[i](x)\n",
        "        x = self.activation(x)  # Attivazione\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "TjMjTZNo7yW-"
      },
      "outputs": [],
      "source": [
        "#per file yaml\n",
        "class VGAEEncoder(torch.nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, output_dim, num_layers, activation, normalization, use_embedding, pre_norm, residual_connections, scale_factor, layer_type):\n",
        "        super(VGAEEncoder, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        self.normalizations = torch.nn.ModuleList()\n",
        "        self.activation = activation_functions[activation]\n",
        "        self.use_embedding = use_embedding\n",
        "        self.pre_norm_cond = pre_norm\n",
        "        self.residual_connections = residual_connections\n",
        "        self.scale_factor = scale_factor\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.normalization = normalization\n",
        "        self.ConvLayer = layer_types[layer_type]\n",
        "        last = False\n",
        "\n",
        "        if self.use_embedding:\n",
        "          self.class_embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
        "          input_dim = embedding_dim\n",
        "\n",
        "        self.layers.append(self.ConvLayer(input_dim, self.compute_new_dim(input_dim, last)))\n",
        "        curr_dim = self.compute_new_dim(input_dim, last)\n",
        "\n",
        "        if self.pre_norm_cond and self.use_embedding:\n",
        "          if self.normalization == \"batch\":\n",
        "            self.pre_norm = BatchNorm1d(embedding_dim)\n",
        "          elif self.normalization == \"layer\":\n",
        "            self.pre_norm = LayerNorm(embedding_dim)\n",
        "\n",
        "        for i in range(num_layers - 1):\n",
        "          if self.normalization == \"batch\":\n",
        "            self.normalizations.append(BatchNorm1d(curr_dim))\n",
        "          elif self.normalization == \"layer\":\n",
        "            self.normalizations.append(LayerNorm(curr_dim))\n",
        "          elif self.normalization == \"none\":\n",
        "            self.normalizations.append(None)\n",
        "          else:\n",
        "            raise ValueError(f\"Unsupported normalization type: {normalization}\")\n",
        "          if self.residual_connections:\n",
        "            if self.normalization == \"batch\":\n",
        "              self.normalizations.append(BatchNorm1d(curr_dim))\n",
        "            elif self.normalization == \"layer\":\n",
        "              self.normalizations.append(LayerNorm(curr_dim))\n",
        "          if (i == num_layers - 2):\n",
        "            last = True\n",
        "          self.layers.append(self.ConvLayer(curr_dim, self.compute_new_dim(curr_dim, last)))\n",
        "          curr_dim = self.compute_new_dim(curr_dim, last)\n",
        "\n",
        "        self.conv_mu = self.ConvLayer(curr_dim, int(curr_dim / 2))\n",
        "        self.conv_logstd = self.ConvLayer(curr_dim, int(curr_dim / 2))\n",
        "\n",
        "  def compute_new_dim(self, current_dim, last):\n",
        "    if last:\n",
        "        return self.output_dim\n",
        "    if current_dim < self.output_dim:\n",
        "        return self.embedding_dim\n",
        "    if self.scale_factor == \"half\":\n",
        "        return int((current_dim + self.output_dim)/2)\n",
        "    else:\n",
        "        return current_dim\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    if self.use_embedding:\n",
        "      node_classes = torch.argmax(x, dim=1)\n",
        "      x = self.class_embedding(node_classes)\n",
        "    if self.pre_norm_cond and self.use_embedding:\n",
        "      x = self.pre_norm(x)\n",
        "    if self.residual_connections:\n",
        "      for i, conv in enumerate(self.layers):\n",
        "          x = x + self.activation(conv(self.normalizations[i](x), edge_index))\n",
        "    else:\n",
        "      for i, conv in enumerate(self.layers):\n",
        "          x = conv(x, edge_index)\n",
        "      if i < len(self.layers) - 1:\n",
        "        if self.normalizations[i] is not None:  # Normalizzazione, se definita\n",
        "            x = self.normalizations[i](x)\n",
        "        x = self.activation(x)  # Attivazione\n",
        "    return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "T8x98oTmvBoa"
      },
      "outputs": [],
      "source": [
        "def corruption(x, edge_index):\n",
        "    return x[torch.randperm(x.size(0), device=x.device)], edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MKj7ellV6msO"
      },
      "outputs": [],
      "source": [
        "model_classes = {\n",
        "    \"GAE\": GAE,\n",
        "    \"VGAE\": VGAE,\n",
        "    \"DGI\": DeepGraphInfomax\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "e1CBueafaZPA"
      },
      "outputs": [],
      "source": [
        "def load_model(model_config):\n",
        "    model_name = model_config[\"model_name\"]\n",
        "    if model_name not in model_classes:\n",
        "        raise ValueError(f\"Modello '{model_name}' non supportato!\")\n",
        "\n",
        "    embedding_dim = model_config[\"embedding_dim\"]\n",
        "    output_dim = model_config[\"output_dim\"]\n",
        "    num_layers = model_config[\"num_layers\"]\n",
        "    activation = model_config[\"activation\"]\n",
        "    normalization = model_config[\"normalization\"]\n",
        "    use_embedding = model_config[\"use_embedding\"]\n",
        "    pre_norm = model_config[\"pre_norm\"]\n",
        "    residual_connections = model_config[\"residual_connections\"]\n",
        "    scale_factor = model_config[\"scale_factor\"]\n",
        "    layer_type = model_config[\"layer_type\"]\n",
        "\n",
        "    if model_name == \"VGAE\":\n",
        "        encoder = VGAEEncoder(\n",
        "            input_dim=data.num_features,\n",
        "            embedding_dim=embedding_dim,\n",
        "            output_dim=output_dim,\n",
        "            num_layers=num_layers,\n",
        "            activation=activation,\n",
        "            normalization=normalization,\n",
        "            use_embedding=use_embedding,\n",
        "            pre_norm=pre_norm,\n",
        "            residual_connections=residual_connections,\n",
        "            scale_factor=scale_factor,\n",
        "            layer_type=layer_type\n",
        "        )\n",
        "    else:  # Per GAE e DGI usiamo l'encoder GAE\n",
        "        encoder = GAEEncoder(\n",
        "            input_dim=data.num_features,\n",
        "            embedding_dim=embedding_dim,\n",
        "            output_dim=output_dim,\n",
        "            num_layers=num_layers,\n",
        "            activation=activation,\n",
        "            normalization=normalization,\n",
        "            use_embedding=use_embedding,\n",
        "            pre_norm=pre_norm,\n",
        "            residual_connections=residual_connections,\n",
        "            scale_factor=scale_factor,\n",
        "            layer_type=layer_type\n",
        "        )\n",
        "\n",
        "    # Se il modello è DGI, usiamo DeepGraphInfomax con il giusto encoder\n",
        "    if model_name == \"DGI\":\n",
        "        return DeepGraphInfomax(\n",
        "            encoder=encoder,\n",
        "            hidden_channels=output_dim,\n",
        "            summary=lambda z, *args, **kwargs: z.mean(dim=0).sigmoid(),\n",
        "            corruption=corruption,\n",
        "        )\n",
        "\n",
        "    return model_classes[model_name](encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "eeAGGojB7pF6"
      },
      "outputs": [],
      "source": [
        "model_names = [\"model1\",\"model2\"]\n",
        "models = []\n",
        "for model_name in model_names:\n",
        "  models.append(load_model(config[model_name]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4rNacuTCxtt",
        "outputId": "f5681ed1-914f-474b-e562-2d4a11ff3922"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VGAE(\n",
              "  (encoder): VGAEEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): GCNConv(5, 256)\n",
              "      (1): GCNConv(256, 128)\n",
              "    )\n",
              "    (normalizations): ModuleList(\n",
              "      (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (activation): GELU(approximate='none')\n",
              "    (conv_mu): GCNConv(128, 64)\n",
              "    (conv_logstd): GCNConv(128, 64)\n",
              "  )\n",
              "  (decoder): InnerProductDecoder()\n",
              ")"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJxTd7Upe5sZ",
        "outputId": "c266caa0-7a6c-44eb-dd11-63893a543a22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-c31190d851be>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  models[0] = torch.load(\"/content/Graph-Machine-Learning/Models/new/model_VGAE_1024_batchSize_batch_35nxn_128_out.pth\")\n"
          ]
        }
      ],
      "source": [
        "models[0] = torch.load(\"/content/Graph-Machine-Learning/Models/new/model_VGAE_1024_batchSize_batch_35nxn_128_out.pth\")\n",
        "#models[1] = torch.load(\"/content/Graph-Machine-Learning/Models/new/model_DGI_1024_batchSize_batch_35nxn_128_out.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W99s2KteouRK",
        "outputId": "7d79987c-9eb3-4705-e62e-1902a95eb61d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-44-5522334dd0bc>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  models[1].load_state_dict(torch.load(f'/content/Graph-Machine-Learning/Models/VGAE/with_embedding/with_pre_norm/with_residual_connections/1024_batchSize_layer_swiglu_35nxn_embed_128_out.pth'))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#models[0].load_state_dict(torch.load(f'/content/Graph-Machine-Learning/Models/new/model_GAE_1024_batchSize_batch_35nxn_128_out.pth'))\n",
        "models[1].load_state_dict(torch.load(f'/content/Graph-Machine-Learning/Models/VGAE/with_embedding/with_pre_norm/with_residual_connections/1024_batchSize_layer_swiglu_35nxn_embed_128_out.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uLFqCz11OLO",
        "outputId": "e88e5fa0-7d6c-4c24-e583-84e556114a15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VGAE(\n",
              "  (encoder): VGAEEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x GCNConv(128, 128)\n",
              "    )\n",
              "    (normalizations): ModuleList(\n",
              "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (activation): GELU(approximate='none')\n",
              "    (class_embedding): Embedding(5, 128)\n",
              "    (pre_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv_mu): GCNConv(128, 64)\n",
              "    (conv_logstd): GCNConv(128, 64)\n",
              "  )\n",
              "  (decoder): InnerProductDecoder()\n",
              ")"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "HPccRXeI-WmE"
      },
      "outputs": [],
      "source": [
        "device = torch.device ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for i in range (len(models)):\n",
        "  models[i] = models[i].to(device)\n",
        "x = data.x.to(device)\n",
        "edge_index = data.edge_index.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "f1P9snB5v0A9"
      },
      "outputs": [],
      "source": [
        "model1_name = config['model1']['model_name']\n",
        "model2_name = config['model2']['model_name']\n",
        "embeddings_list = [None] * len(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "K0OvJ4jm-MT2"
      },
      "outputs": [],
      "source": [
        "for i,model in enumerate(models):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    if config['model' + str(i+1)][\"model_name\"] == \"DGI\":\n",
        "      embeddings_list[i], _, _ = model(x, edge_index)\n",
        "    else:\n",
        "      embeddings_list[i] = model.encode(x, edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCXXV_JID-uz",
        "outputId": "e7ffdd3e-a0b1-49ac-ebdc-1829c58afe76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[-0.1828, -0.0271,  0.0843,  ..., -0.1121, -0.2734,  0.1016],\n",
              "         [-0.1828, -0.0271,  0.0843,  ..., -0.1121, -0.2734,  0.1016],\n",
              "         [-0.1828, -0.0271,  0.0843,  ..., -0.1121, -0.2734,  0.1016],\n",
              "         ...,\n",
              "         [ 0.0056,  0.0327,  0.0272,  ..., -0.0198,  0.0220,  0.0193],\n",
              "         [-0.0059,  0.0142,  0.0236,  ..., -0.0167,  0.0272,  0.0216],\n",
              "         [ 0.0059,  0.0325,  0.0264,  ..., -0.0193,  0.0211,  0.0185]],\n",
              "        device='cuda:0'),\n",
              " tensor([[ 0.0446,  0.0097,  0.1104,  ...,  0.0187, -0.1304, -0.0246],\n",
              "         [ 0.0446,  0.0097,  0.1104,  ...,  0.0187, -0.1304, -0.0246],\n",
              "         [ 0.0446,  0.0097,  0.1104,  ...,  0.0187, -0.1304, -0.0246],\n",
              "         ...,\n",
              "         [ 0.2501, -0.1115,  0.8424,  ..., -0.1767,  0.2185,  0.0936],\n",
              "         [ 0.2419, -0.1242,  0.5545,  ..., -0.1468,  0.1801, -0.0024],\n",
              "         [ 0.2550, -0.1100,  0.8487,  ..., -0.1782,  0.2212,  0.0896]],\n",
              "        device='cuda:0')]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3_RAiMXh_dum"
      },
      "outputs": [],
      "source": [
        "num_samples = 5000\n",
        "subset_indices = torch.randperm(embeddings_list[0].size(dim = 0))[:num_samples]\n",
        "sample_1 = embeddings_list[0][subset_indices]\n",
        "sample_2 = embeddings_list[1][subset_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xIdijOeAGeW",
        "outputId": "721c2dc2-1222-45d8-91a0-d190d404021f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          mutual_knn: 0.259 [elapsed: 0.06s]\n",
            "                 cka: 0.654 [elapsed: 0.66s]\n",
            "               cknna: 0.169 [elapsed: 0.22s]\n",
            "Total time: 9.40s\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "sample_1 = F.normalize(sample_1, dim=-1)\n",
        "sample_2 = F.normalize(sample_2, dim=-1)\n",
        "\n",
        "import time\n",
        "trials = 10\n",
        "\n",
        "t0 = time.time()\n",
        "for metric in AlignmentMetrics.SUPPORTED_METRICS:\n",
        "\n",
        "    scores, times = [], []\n",
        "    for t in range(trials):\n",
        "        t_st = time.time()\n",
        "\n",
        "        kwargs = {}\n",
        "        if 'nn' in metric:\n",
        "            kwargs['topk'] = 10\n",
        "        if 'cca' in metric:\n",
        "            kwargs['cca_dim'] = 10\n",
        "        if 'kernel' in metric:\n",
        "            kwargs['dist'] = 'sample'\n",
        "\n",
        "        score = AlignmentMetrics.measure(metric, sample_1, sample_2, **kwargs)\n",
        "        scores.append(score)\n",
        "        times.append(time.time() - t_st)\n",
        "    print(f\"{metric.rjust(20)}: {np.mean(scores):1.3f} [elapsed: {np.mean(times):.2f}s]\")\n",
        "\n",
        "print(f'Total time: {time.time() - t0:.2f}s')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
